\section{Inferring port-Hamiltonian Systems}\label{sec:inferring-ph-systems}

We introduce several methods to infer port-Hamiltonian systems in a nonintrusive manner throughout this section.
First, we commence by a hybrid approach in Subsection~\ref{subsec:realization-based-inference}: We apply a nonintrusive method to obtain an \ac{LTI} and afterwards compute a port-Hamiltonian realization from it.
Lastly, there exist a number of optimization-based inference methods similar to the problems in Section~\ref{sec:mor-quadratically-embedded-manifolds} which we highlight in Subsection~\ref{subsec:optimization-based-inference}.

\subsection{Realization-Based Inference}\label{subsec:realization-based-inference}

Realization-based Operator Inference works as a two step procedure:
\begin{enumerate}
    \item Use the provided data $x_i, u_i, y_i, i = 1, \dots, k$ to infer an \ac{LTI} realization $\Sigma_\msc{lti} \colon (A, B, C, D, E)$, and
    \item Compute a port-Hamiltonian realization $\Sigma_\msc{ph}$ from $\Sigma_\msc{lti}$.
\end{enumerate}
This approach thus is inherently hybrid, because the second step requires access to the system matrices, and cannot be considered purely nonintrusive.
There are also quite a few downsides to this way of approximating a system.
If the second step is not very robust with respect to the input data, then even small numerical errors or noise in the measurements can cause the result to be nonsensical.
We thus only consider this framework as a theoretical stepping stone to introduce later more direct ideas.

\itodo{talk about advantages of skipping intermediaries, or somehow give a reason why we would want to concern ourselves with these methods and not just the one we use at the end\dots}

\itodo{move peherstorfer paragraph from previous section here!} % Operator inference for ltis with potential quadratic terms~\cite{Peherstorfer2016}

Alternatively, there exists the paradigm of the \emph{Loewner method}.
The Loewner method uses samples of the transfer function and the corresponding tangential directions to construct system matrices.
We commence as described in~\cite{BGD2020} with the transfer function $H$ of an \ac{LTI} $\Sigma_\msc{lti}$ and the interpolation points
\begin{equation*}
    v_i\trans \coloneqq  l_i\trans H(\mu_i),\quad w_i \coloneqq H(\lambda_i) r_i,\quad D\coloneqq H(\infty),
\end{equation*}
such that the interpolation conditions fulfill $l_i, r_i, v_i, w_i \in \bb{C}^n, \lambda_i, \mu_i \in \bb{C}$.
Then we can construct the Loewner and the shifted Loewner matrix
\begin{equation}\label{eq:loewner-matrices}
    \bb{L} \coloneqq \begin{pmatrix}
        \frac{l_1 w_1  - v_1 r_1}{\lambda_1 - \mu_1} & \cdots & \frac{l_1 w_n  - v_1 r_n}{\lambda_n - \mu_1} \\
        \vdots & \ddots & \vdots \\
        \frac{l_n w_1  - v_n r_1}{\lambda_1 - \mu_n} & \cdots & \frac{l_n w_n  - v_n r_n}{\lambda_n - \mu_n}
    \end{pmatrix},\quad \bb{L}_\sigma \coloneqq \begin{pmatrix}
        \frac{\lambda_1 l_1 w_1 - \mu_1 r_1 v_1}{\lambda_1 - \mu_1} & \cdots & \frac{\lambda_n l_1 w_n - \mu_1 r_n v_1}{\lambda_n - \mu_1} \\
        \vdots & \ddots & \vdots \\
        \frac{\lambda_1 l_n w_1 - \mu_n r_1 v_n}{\lambda_1 - \mu_n} & \cdots & \frac{\lambda_n l_n w_n - \mu_n r_n v_n}{\lambda_n - \mu_n}
    \end{pmatrix}
\end{equation}
If the Loewner matrix is invertible, then according to~\cite[Theorem~5.1]{BGD2020} we can use $\bb{L}, \bb{L}_\sigma, L \coloneqq {(l_1, \dots, l_n)}\trans, R \coloneqq (r_1, \dots, r_n), M \coloneqq \diag{\mu_1, \dots, \mu_n}, N \coloneqq \diag{\lambda_1, \dots, \lambda_n}, V = {(v_1, \dots, v_n)}\trans, W \coloneqq (w_1, \dots, w_n)$ to create a realization of $\Sigma_\msc{lti}$ with the following block structure
\begin{equation}\label{eq:loewner-realization}
    \begin{pmatrix}
        A - sE & B \\
        C & D
    \end{pmatrix} = \begin{pmatrix}
        \bb{L}_\sigma - s \bb{L} & V \\
        -W & 0
    \end{pmatrix} + \begin{pmatrix}
        -L \\
        \id
    \end{pmatrix} D \begin{pmatrix}
        R & \id
    \end{pmatrix}.
\end{equation}
This basic procedure can be amended and extended in various ways.
Choosing a specific set of interpolation points such as the set of spectral zeros of the transfer function $H$ can yield additional properties which directly allow the construction of a port-Hamiltonian system, cf.~\cite{BGD2020, Poussot2022}.
Instead of directly sampling the transfer function we can also apply transformations such as the Discrete Fourier Transform to time domain data, cf.~\cite{Peherstorfer2017, Cherifi2021}.
Other authors have also directly incorporated higher order terms into the theoretical aspects, for example by making use of the fact that a system's Loewner matrices are generalized observability and controllability Gramians, cf.~\cite{Antoulas2019}, or by iteratively computing a separate higher order model which fits the systemic error onto the higher order terms, cf.~\cite{GKA2021}.

\itodo{the last thought is very close to our procedure, remark on that?}

% loewner
% loewner with estimation of spectral zeros~\cite{BGD2020}
% builds on Benner~\cite{BGD2020}, modifies for non strict passivity~\cite{Poussot2022}; their problems are very similar to mine!
% Time domain loewner~\cite{Peherstorfer2017}
% time domain loewner via dft, hints on how large of a frequency space can be estimated for set of number of samples~\cite{Cherifi2022}
% Loewner for linear and bilinear-quadratic systems~\cite{Antoulas2019}
% loewner multi model method with several transfer functions and iterative algorithm to match quadratic part to data of oscillatory inputs~\cite{GKA2021}

In a similar fashion to the Least Squares problem in Section~\ref{sec:mor-quadratically-embedded-manifolds}, we can formulate a similar approach to fit operators in \acp{LTI}.
This idea relies on \emph{Dynamic Mode Decomposition} (DMD) as well as its extension Input-Output DMD (ioDMD) as introduced in~\cite{Annoni2016}.
For an \ac{LTI} $\Sigma_\msc{lti}$ with a known matrix $E \in \bb{R}^{n \times n}$ under the control variable $u \in \bb{R}^m$ we concatenate the individual data snapshots for time points $t_i, i = 1, \dots, k$ into the matrices
\begin{equation}\label{eq:iodmd-data-matrices}
    \Omega \coloneqq \begin{pmatrix}
        x_1 & \cdots & x_k \\
        u_1 & \cdots & u_k
    \end{pmatrix},\quad \Gamma \coloneqq \begin{pmatrix}
        E \dot{x}_1 & \cdots & E \dot{x}_k \\
        y_1 & \cdots & y_k
    \end{pmatrix}.
\end{equation}
This allows us to write the entire \ac{LTI} as $\Gamma = \Sigma \Omega$, where $\Sigma$ corresponds to the block matrix containing the remaining system matrices of $\Sigma_\msc{lti}$, cf.~\cite{Heiland2022}.
The usual Least Squares problem
\begin{equation*}
    \min\limits_{\Sigma \in \bb{R}^{n + m \times n + m}} \norm{\Gamma - \Sigma \Omega}{F}^2
\end{equation*}
is solved by $\Sigma = \Gamma \Omega^\dagger$, whereby $\Omega^\dagger$ denotes the pseudo-inverse of $\Omega$.
The ioDMD framework can further be extended to bilinear and quadratic terms by appropriately amending the data matrices, cf.~\cite{Gosea2021}.

% dmd
%fitting structured data with dmd/least squares formulations~\cite{Gosea2021}
% overview article without direct connection to ltis~\cite{Heiland2022}

Another way of inferring \acp{LTI} is by measuring the output producec by a unit step input at the initial time point, cf.~\cite{Miller2012}.
The essential idea in this approach is to estimate the system's transfer function and by extend its Gramians, somewhat reflecting the procedure of the time domain Loewner method.
Afterwards, a realization is assembled using an SVD-based computation.

Finally, as a last method to come up with an initial \ac{LTI} realization, we propose to extend the \ac{SINDy} framework.
\ac{SINDy} is based on a function library-based approach, which has been extended to implicit dynamical systems~\cite{Mangan2016, Kaheman2020} and systems with control terms~\cite{Kaiser2018}.
The principle formulation of \ac{SINDy} as given in~\cite{Brunton2016} acts on dynamical systems of the form
\begin{equation}\label{eq:sindy-explicit-dynamical-system}
    \frac{\dif}{\dif t} x = f(x).
\end{equation}
Next, we construct the state data matrices $X \coloneqq (x_1, \dots, x_k), \dot{X} \coloneqq (\dot{x}_1, \dots, \dot{x}_k)$ and apply transformations to form the function library
\begin{equation}\label{eq:sindy-explicit-library}
    \Theta(X) = (\theta_1(X), \dots, \theta_\ell(X)).
\end{equation}
Similarly to DMD, we fit the transformed data such that
\begin{equation}\label{eq:sindy-explicit-fit}
    \dot{X} = P \Theta(X),
\end{equation}
where $P \in \bb{R}^{n \times n}$ is a sparse matrix of coefficients reflecting which library components have been selected in the final model.
The solution $P$ has to be computed with a sparsity promoting solver such as Sequentially Thresholded Least Squares (LTLSQ)~\cite{Zhang2019}, the LASSO framework~\cite{Tibshirani1996}, or other similar methods as mentioned in~\cite{Kaiser2018, Kaheman2020}.
To start to fit \acp{LTI} with \ac{SINDy}, we have to convert the explicit system~\eqref{eq:sindy-explicit-dynamical-system} as well as the corresponding fitting equation~\eqref{eq:sindy-explicit-fit} into the implicit formulation
\begin{equation}\label{eq:sindy-implicit-equations}
    \begin{aligned}
        f(x, \dot{x}) &= 0, \\
        P \Theta(X, \dot{X}) &= 0.
    \end{aligned}
\end{equation}
This transformation yields the implicit-SINDy~\cite{Mangan2016} or \ac{SINDy-PI}~\cite{Kaheman2020} methods, with the latter having demonstrated better performance results.
This slightly changes the conditions on the sparse coefficient matrix $P$, requiring that $\diag{P} = 0$ because otherwise $P = \id$ would be a trivial solution, cf.~\cite[Section~3~(b)]{Kaheman2020}.
Incorporating control variables into the \ac{SINDy} framework has been accomplished by including appropriate control and mixed state-control terms into the function library~\eqref{eq:sindy-explicit-library}, cf.~\cite{Kaiser2018}, yielding the so-called \ac{SINDYc} procedure, which under certain conditions equals DMDc.
Unlike the procedure of DMDc we propose to proceed with \ac{SINDYc} but instead consider stacked variables instead of the usual extended library.
This results in the explicit system
\begin{equation}\label{eq:sindy-io}
    f(x, u) = P \Theta \begin{pmatrix}
        X \\
        U
    \end{pmatrix} = \begin{pmatrix}
        E \\
        \id
    \end{pmatrix} \begin{pmatrix}
        \dot{x} & y
    \end{pmatrix},
\end{equation}
where we have to adapt $\Theta$ such that the library allows for transformations on certain data components.
Thereafter, we can extract the matrix blocks from the coefficient matrix $P$ as the system matrices $A, B, C, D$ by selecting the appropriate row and column indices.
As an extension to the linear library we would employ if we were to fit an \ac{LTI}, we could also directly choose quadratic, other higher order polynomial, or even nonpolynomial nonlinear transformations, thus direclty affecting the objective of this thesis, however we postpone this discussion to Section~\ref{sec:quadratically-embedded-manifolds-ph-systems}.
We also have to remark that while we can easily repurpose \ac{SINDy} to procure an \ac{LTI} realization, using this framework to compute port-Hamiltonian realizations is much more difficult as a cause of the structural matrix constraints.
It would certainly be possible to find a depiction of the optimizational constraints for a \ac{PH} realization, however the efficiency of the plain solvers as LTLSQ or LASSO would be lost alongside the convexity of the underlying optimization problem.
Some authors have proposed remedies to this restriction, however their methods once again involve Neural Networks, therefore incurring the explainability issue, cf.~\cite{Lee2022}.

\itodo{we don't actually need \ac{SINDy-PI}, remove it!}

% sindy
% first paper on sindy~\cite{Brunton2016}
% sindyc~\cite{Kaiser2018}
% implicit sindy~\cite{Kaheman2020}
% structure preserving sindy (especially pH) with nns~\cite{Lee2022}

As soon as we have constructed an \ac{LTI} $\Sigma_\msc{lti}$, we can manipulate it until we are left with a port-Hamiltonian system.
To this end, there are again various methods available in the literature, and we only discuss a few that are closely related to the concepts we brought up so far.
It is imperative that we again mention the similarities between stable, positive real, and port-Hamiltonian systems as already mentioned in Subsection~\ref{subsec:prbt} and~\cite{Cherifi2022}, because most of the ideas to convert \acp{LTI} to \ac{PH} systems are closely linked to these concepts.

First off, we discuss the results from~\cite{Beattie2022}.
In this publication they explicitly derive properties that an \ac{LTI} needs to fulfill such that one can compute a port-Hamiltonian realization from $\Sigma_\msc{lti} = (A, B, C, D, \id)$.
They use the real KYP linear matrix inequality~\eqref{eq:kyp-lmi}
\begin{equation*}
    \begin{pmatrix}
        - A\trans Q - Q A & C\trans - QB \\
        C - B\trans Q & D + D\trans
    \end{pmatrix} \succcurlyeq 0.
\end{equation*}
Afterwards, they use the symmetric positive definite solution $\tilde{Q}$ to derive a regular transformation matrix $T$ from a Cholesky decomposition $\tilde{Q} = T\trans T$ and show that the transformed system
\begin{equation}\label{eq:ph-transformation-from-passive-system}
    Q \coloneqq \id,\quad J \coloneqq \frac{1}{2} (T A T\inv - {(T A T\inv)}\trans),\quad R \coloneqq -\frac{1}{2} (T A T\inv + {(T A T\trans)}\trans),\quad G \coloneqq \frac{1}{2} (T B + {(C T\inv)}\trans),\quad P \coloneqq \frac{1}{2} (T B + {(C T\inv)}\trans),\quad S \coloneqq \frac{1}{2} (D + D\trans),quad N \coloneqq \frac{1}{2} (D - D\trans)
\end{equation}
is indeed port-Hamiltonian.
This method is feasible only for small to medium scale problems, cf.~\cite{Cherifi2019}, leading them to propose two other methods.
The first of these proposes to use a similar procedure based on a positive real balanced truncation, resulting in much better scaling with respect to large model orders.
The second method takes a completely different angle: optimizing over the matrices to obtain a best fit oh a port-Hamiltonian system to the original \ac{LTI} $\Sigma_\msc{lti}$.
This change of perspective opens up a set of optimization-based methods, which we discuss in Subsection~\ref{subsec:optimization-based-inference}.

% ph system computation
% three approaches to compute ph from lti~\cite{Cherifi2019}; 1: unstructured approach via lmis 2: based on prbt 3: nearest ph system via optimization
%computation of ph system through ares and lyapunov eqs for stability~\cite{Beattie2022}

\subsection{Optimization-Based Inference}\label{subsec:optimization-based-inference}

This subsections deals with optimization-based port-Hamiltonian operator inference methods.
In essence, all of these methods choose some parametrization of the underlying system matrices and then run an optimization algorithm to minimize a chosen cost functional.
The choice of algorithm and the construction of the individual optimization objectives and the inclusion of structural constraints heavily varies between publications.
As a final point one has to consider the initialization of the matrices.
Most of the algorithms are very succeptible to initial value changes because the optimization problems are generally neither convex nor linear.

The first method we mention is the \emph{Nearest Port-Hamiltonian System}, cf.~\cite{Gillis2018} and~\cite{Cherifi2019} respectively, where the connection between the two is immediate by the last comments of the preceding subsection and by Subsection~\ref{subsec:prbt}.
The main minimization problem is given by the relation to the inferred \ac{LTI} $\Sigma_\msc{lti} = (A, B, C, D, E)$
\begin{equation}\label{eq:nearest-ph-system-minimization-problem}
    \begin{aligned}
        &\min\limits_{F, J, R, G, P, S, N} \mcl{J}(F, J, R, G, P, S, N) \coloneqq \norm{A - (J - R)}{F}^2 + \norm{B - (G - P)}{F}^2 + \norm{C - {(G + P)}\trans}{F}^2 + \norm{D - (S - N)}{F}^2 + \norm{E - F}{F}^2 \\
        &\suchthat -J = J\trans, -N = N\trans, R = R\trans \succcurlyeq 0, S = S\trans \succcurlyeq 0, E = E\trans \succcurlyeq 0, \begin{pmatrix}
            R & P \\
            P\trans & S
        \end{pmatrix} \succcurlyeq 0.
    \end{aligned}
\end{equation}
In~\cite{Gillis2018}, this problem is solved with the \emph{Fast Gradient Method} in combination with a restarting mechanism to stabilize convergence properties of the algorithm.
Along with these features, the objective function~\eqref{eq:nearest-ph-system-minimization-problem} allows for weights on each individual summand prioritizing certain system terms during the optimization.
Besides this advantage, the computation of the cost function gradient is very easy for the Frobenius norm $\nabla_X \norm{A X - B}{F}^2 = 2 A\trans (A X - B)$.
As far as initialization is concerned, the authors of~\cite{Gillis2018} propose two methods: Initialize with additional assumptions on some matrices such that the optimal solution can be given analytically, and solve an LMI to compute an initial port-Hamiltonian realization along the same lines of arguments as previous links between LMIs and positive real systems have done.

% optimization based projection of admissible lti system as ph~\cite{Gillis2018}; includes conversion from an lti system $(A, B, C, D, E)$ to ph, restarting FGM; this is similar to~\cite[Section~5]{Cherifi2019}

Another FGM related method is the \emph{Port-Hamiltonian Dynamic Mode Decomposition} (pH DMD), cf.~\cite{Morandin2022}.
As the name implies this is closely linked to the DMD method in that we want to minimize
\begin{equation}\label{eq:ph-dmd-minimization-problem}
    \norm{\begin{pmatrix}
        E \dot{X} \\
        -Y
    \end{pmatrix} - (\mcl{J} - \mcl{R}) \begin{pmatrix}
        X \\
        U
    \end{pmatrix}}{F}^2,
\end{equation}
under the constraints that the system block matrices fulfill $-\mcl{J} = \mcl{J}\trans$ and $\mcl{R} = \mcl{R}\trans \succcurlyeq 0$, where $X, \dot{X}, U$ and $Y$ are data matrices which in this case include adjustments made via implicit midpoint rules.
Their method differentiates itself from the previous FGM by the fact that their algorithm does not allow for restarting, however they also do not offer the ability of optimizing the $E$ matrix at the same time because in this setting it would lead to the trivial minimizer $0$,cf.~\cite[Remark~3.3]{Morandin2022}.
On the other hand, pH DMD improves the way they can handle different modes of initialization.
If either $\mcl{J}$ or $\mcl{R}$ are given, then the other matrix can be explicitly computed.
This allows for a step wise update of the matrices within the FGM algorithm.
Further, by considering the weighted Least Squares problem
\begin{equation*}
    \min \norm{\begin{pmatrix}
        X & U
    \end{pmatrix} (\begin{pmatrix}
        E \dot{X} \\
        -Y
    \end{pmatrix} - (\mcl{J} - \mcl{R}) \begin{pmatrix}
        X \\
        U
    \end{pmatrix})}{F}^2
\end{equation*}
provides an initial approximate solution to the unweighted problem with an estimate bounding the error of the unweighted problem by the weighted version, cf.~\cite[Lemma~3.11]{Morandin2022}.
One downside of this method is the inherent formulation using a fixed time stwpping scheme: pH DMD is based of implicit midpoint stepping and choosing other time stepping schemes such as RK45 may decrease performance of the algorithm despite the time scheme being of a higher order.

% FGM derivative algorithm, no restarting allowed; initializes from weighted least squares problem~\cite{Morandin2022}

Instead of choosing a Least Squares approach which fits both the output data and the state data, we can also consider a calibration problem
\begin{equation}\label{eq:günther-calibration-objective}
    \mcl{J}(y, \omega) \coloneqq \frac{1}{2} \int\limits_0^T \abs{y(t) - y_\msc{data}(t)}{}^2 \dif t + \frac{\lambda}{2} \abs{\omega - \omega_\msc{ref}}{}^2, \quad \lambda \geq 0
\end{equation}
where $\omega = (J, R, Q, G, P, x_0)$ is a vectorized representation of the system matrices and the initial data as described in~\cite{Günther2023}.
The authors compute the gradient of the objective functional by considering the error in the two states and computing the first order optimality conditions with Fréchet derivatives.
Along the way they also derive an adjoint equation and use standard techniques to state a gradient descent algorithm.
The downside of this approach is that we have to solve the system at every step of the algorithm.
While this may be feasible on small scale or reduced order models, it does not pose well for models of larger scales such as the quadratic models we want to consider later on, whose order scales polynomially.

% parametrized gradients via adjoint equations with block matrices, robust optimization condition analysis, no interesting numerical experiments~\cite{Günther2023}

Instead of optimizing over e.\@ g.\@ symmetric positive (semi-) definite matrices at once it would also be reasonable to decompose all structure-constrained matrices into distinctive, basis-like elements such that we can simply adjust the coefficients through a parameter optimization with simpler constraints.
To this end we highlight a couple papers which use this idea in slightly different ways.

Firstly, for a port-Hamiltonian system of the form
\begin{equation*}
    \begin{pmatrix}
        \dot{x} \\
        y
    \end{pmatrix} = (\mcl{J} - \mcl{R}) \begin{pmatrix}
        \nabla_x H(x) \\
        u
    \end{pmatrix}
\end{equation*}
we can deconstruct an exemplary skew-symmetric matrix $J \in \bb{R}^{3 \times 3}$ as
\begin{equation}\label{eq:skew-symmetric-basis-decomposition}
    N(\omega) = \omega_1 \begin{pmatrix}
        0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix} + \omega_2 \begin{pmatrix}
        0 & 0 & 1 \\
        0 & 0 & 0 \\
        -1 & 0 & 0
    \end{pmatrix} + \omega_3 \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & -1 & 0
    \end{pmatrix}.
\end{equation}
Instead of considering the gradient over the entire matrix, we can now consider it over the set of parameters $\omega = (\omega_1, \dots, \omega_p)$ as the vector $\nabla_\omega \mcl{J}$.
Similarly, we deconstruct an exemplary positive definite matrix $R \in \bb{R}^{3 \times 3}$ in a similar fashion to~\eqref{eq:skew-symmetric-basis-decomposition} as
\begin{equation}\label{eq:spsd-basis-decomposition}
    R(\omega) = T(\omega) T(\omega)\trans,\quad T(\omega) = \omega_1 \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix} + \omega_2 \begin{pmatrix}
        0 & 0 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix} + \omega_3 \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 0
    \end{pmatrix} + \omega_4 \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & 0 \\
        1 & 0 & 0
    \end{pmatrix} + \omega_5 \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & 0 \\
        0 & 1 & 0
    \end{pmatrix} + \omega_6 \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}.
\end{equation}
Following along with~\cite{Najnudel2021}, we can also parametrize the Hamiltonian $H$ (in our case represented solely by $E$) using a kernel estimation with a parameter and compute the gradient of the error $\norm{\Sigma_\msc{ph}(\theta) - \Sigma_\msc{ph}}{F}^2$ by analytical computations.
The authors then use an \emph{Interior Points Method} with an additional logarithmic barrier in the objective function.
The barrier penalizes the matrix coefficients if they do not ressemble a positive definite matrix, hence this method relies on a weakly enforced constraint.

\itodo{look into the details of this optimization, this cannot remain this vague!}
\itodo{using $\mcl{J}$ as both a system matrix and the optimization objective is a very bad idea!}

% modelling via separating J and R into separate components, but slightly differently from Schwerdtner, inherently approximate the hamiltonian function through kernels, R initialization via least squares~\cite{Najnudel2021}

Unlike the approach from~\cite{Najnudel2021}, the authors of~\cite{Schwerdtner2021, SV2021, Schwerdtner2022, Schwerdtner2023} parametrize the the matrices not as the relevant components of the matrices as in~\eqref{eq:skew-symmetric-basis-decomposition} and~\eqref{eq:spsd-basis-decomposition}, but rather as (strictly) upper triangular and full matrices.
To this end we introduce the vector-to-matrix functions
\begin{align}
    &\vecmat \colon \bb{C}^{n \cdot m} \to \bb{C}^{m \times n}, \theta \mapsto \begin{pmatrix}
        v_1 & \cdots & v_n \\
        \vdots & \ddots & \vdots \\
        v_{(n - 1) \cdot m + 1} & \cdots & v_{n \cdot m}
    \end{pmatrix} \label{eq:v2m}\\
    &\vecupper \colon \bb{C}^{\frac{n (n + 1)}{2}} \to \bb{C}^{n \times n}, \theta \mapsto \begin{pmatrix}
        v_1 & v_2 & \cdots & v_n \\
        0 & v_{n + 1} & \cdots & v_{2n - 1} \\
        0 & 0 & \ddots & \vdots \\
        0 & 0 & 0 & v_{\frac{n (n + 1)}{2}}
    \end{pmatrix} \label{eq:v2u}\\
    &\vecstrict \colon \bb{C}^{\frac{n (n - 1)}{2}} \to \bb{C}^{n \times n}, \theta \mapsto \begin{pmatrix}
        0 & v_1 & v_2 & \cdots & v_{n - 1} \\
        0 & 0 & v_n & \cdots & v_{2n - 2} \\
        0 & 0 & 0 & \ddots & \vdots \\
        0 & 0 & 0 & 0 & v_{\frac{n (n - 1)}{2}} \\
        0 & 0 & 0 & 0 & 0
    \end{pmatrix} \label{eq:v2s}
\end{align}
and their inverse matrix-to-vector functions
\begin{equation}\label{eq:m2v-functions}
    \matvec \colon \bb{C}^{n \times m} \to \bb{C}^{n \cdot m},\quad \uppervec \colon \bb{C}^{n \times n} \to \bb{C}^{\frac{n (n + 1)}{2}},\quad \strictvec \colon \bb{C}^{n \times n} \to \bb{C}^{\frac{n (n - 1)}{2}}.
\end{equation}
From there we can then parametrize the port-Hamiltonian system matrices as follows
\begin{equation}\label{eq:sobmor-parametrization}
    \begin{alignedat}{3}
        E(\theta) &\coloneqq {\vecupper(\theta_\msc{E})}\trans \vecupper(\theta_\msc{E}),\quad &Q(\theta) &\coloneqq {\vecupper(\theta_\msc{Q})}\trans \vecupper(\theta_\msc{Q}) \\
        J(\theta) &\coloneqq {\vecstrict(\theta_\msc{J})}\trans - \vecstrict(\theta_\msc{J}),\quad &R(\theta) &\coloneqq {\vecupper(\theta_\msc{R})}\trans \vecupper(\theta_\msc{R}) \\
        N(\theta) &\coloneqq {\vecstrict(\theta_\msc{N})}\trans - \vecstrict(\theta_\msc{N}),\quad &S(\theta) &\coloneqq {\vecupper(\theta_\msc{S})}\trans \vecupper(\theta_\msc{S}) \\
        G(\theta) &\coloneqq \vecmat(\theta_\msc{G}),\quad &P(\theta) &\coloneqq \vecmat(\theta_\msc{P}).
    \end{alignedat}
\end{equation}
\itodo{mention large scale parameter and connection between $\theta$ and $\theta_\msc{R}$, etc.}
Note that the matrices $R, P$, and $S$ can also be amalgated into a single block matrix $W$ as in~\cite{Schwerdtner2021}, however we have obtained better results in opting for individual parametrizations.
Their algorithm Structured Optimization-Based Model Order Reduction (SOBMOR), cf.~\cite{Schwerdtner2023}, does not use a data-based objective functional such as~\eqref{eq:günther-calibration-objective} from~\cite{Günther2023}, but instead they compute the error as a sum of singular values from a error matrix $\mcl{H}(s_i) - \mcl{H}_\theta(s_i)$, where $s_i$ are sets of sample points, and $\mcl{H}$ and $\mcl{H}_\theta$ denote the transfer functions of the full order model $\Sigma_\msc{ph}$ as well as the reduced system $\hat{\Sigma}_\msc{ph}$.
This error term directly relates the optimized system to the $\mcl{H}_\infty$ norm from truncation-based model reduction as described in Section~\ref{subsec:balanced-truncation}, however computation of the $\mcl{H}_\infty$ norm is slow in repeated settings as well as for large scale systems, rendering it highly undesirable as an optimization objective.
In~\cite{Schwerdtner2021, SV2021}, they only consider the largest singular value for every datum, however in the later publications~\cite{Schwerdtner2022, Schwerdtner2023} they rely on a fixed number of the largest singular values instead.
Additionally, the latest version of their objective functional uses leveled Least Squares terms, bounding the maximum error at every optimization step, and updating the bound via either specifying a fixed sequence or by adaptively performing a bisection algorithm on the bound parameter.

For this thesis, we only give the leveled objective functional on a set of sample points $\mcl{S}$ which includes the $n$ largest singular values
\begin{equation}\label{eq:sobmor-objective}
    \mcl{L}(\gamma, \mcl{H}, \mcl{H}_\theta, \mcl{S}) \coloneqq \frac{1}{\gamma} \sum\limits_{s \in \mcl{S}} \sum\limits_{i = 1}^n {\left( {[\sigma_i(\mcl{H}(s) - \mcl{H}_\theta(s)) - \gamma]}_+ \right)}^2,
\end{equation}
where we define ${[x]}_+ \coloneqq \max \{ 0, x \}$.
In order to compute the gradient of the objective function~\eqref{eq:sobmor-objective} with respect to $\theta$, let $f_i(s; \theta) \coloneqq \sigma_i(\mcl{H}(s) - \mcl{H}_\theta(s))$ and $g(\theta) \coloneqq L(\gamma, \mcl{H}, \mcl{H}_\theta, \mcl{S})$ denote the inner and outer function respectively.
We first calculate the derivative of the outer function, and afterwards the inner derivatives.
For a parameter $\theta \in \bb{R}^N$ we consider the gradient $\nabla_\theta g(\xi) = {(\pd[\theta_1]{} g(\xi), \dots, \pd[\theta_N]{} g(\xi))}\trans$, and compute each of its left and right partial derivatives ($\pd[]{-}$ and $\pd[]{+}$ respectively) by the following equation
\begin{equation}\label{eq:outer-sobmor-objective-gradient}
    \begin{alignedat}{1}
        \pd[\theta_l]{} g(\xi) &= \frac{2}{\gamma} \sum\limits_{f_i(s; \xi) > \gamma} (f_i(s; \xi) - \gamma) \pd[\theta_l]{+} f_i(s; \xi) \\
        &= \frac{2}{\gamma} \sum\limits_{f_i(s; \xi) > \gamma} (f_i(s; \xi) - \gamma) \pd[\theta_l]{-} f_i(s; \xi),\quad l = 1, \dots, N.
    \end{alignedat}
\end{equation}
As noted in~\cite[Remark~3.8]{Schwerdtner2023}, the inner function $f$ in general is not differentiable at $\xi$, however the sum of these singular values still is.
Furthermore, if all singular values represented $f_i(s; \xi)$ greater than $\gamma$ are simple, then the left and right partial derivatives coincide and can be computed explicitly.
Henceforth, we assume that this simplicity condition holds true for all relevant singular values.

\itodo{prove computation of gradient}

In order to compute the gradients of the inner functions $f_i$, we require the following Lemma.

\begin{lemma}[{Cf.~\cite[Lemma~3.4]{Schwerdtner2023} or~\cite[Lemma~3]{Schwerdtner2022}}]\label{lem:v2m-function-trace}
    Let $A \in \bb{C}^{n \times m}$ be a matrix, and $e_i \in \bb{C}^{n \cdot m}$ be the $i$-th standard basis vector.
    Then it holds that
    \begin{equation}\label{eq:v2m-trace-equation}
        \trace{A \vecmat(e_i)} = e_i\trans \matvec(A\trans).
    \end{equation}
    Similar versions of~\eqref{eq:v2m-trace-equation} hold for the case that $A \in \bb{C}^{n \times n}$ is an upper triangular matrix or a strictly upper triangular matrix, and $e_i$ is the $i$-th standard basis vector of appropriate dimension.
    In these cases, $\vecmat$ and $\matvec$ are replaced by $\vecupper$ and $\uppervec$, and $\vecstrict$ and $\strictvec$ respectively.
\end{lemma}

\begin{proof}
    \itodo{Appendix?; cf.~\cite[Lemma~3.4]{Schwerdtner2023} or~\cite[Lemma~3]{Schwerdtner2022}}
    This proof adapts the proof of a very similar statement from~\cite[Lemma~3.4]{Schwerdtner2023}.
    We only demonstrate how the statement holds for the functions $\vecmat$ and $\matvec$.
    Let $A \in \bb{R}^{m, n}$ be a matrix, and consider $e_i$ to be the $i$-th standard basis vector of $\bb{R}^{n \cdot m}$, and suppose that $i$ corresponds to some index pair $(k, l) \in \bb{N}^2$ such that $\vecmat(e_i) = {(\delta_{i, j})}_{i, j = 1}^{n, m} \eqqcolon E$.

    \itodo{fix the portrayal of $E$, this indexing is plain wrong!}

    By consulting the definition of $\matvec$, we see that $\matvec(A\trans) = {(a_{1, 1}, \dots, a_{m, 1}, a_{1, 2}, \dots, a_{m, n})}\trans$.
    Thus, by multiplication we get $e_i\trans \matvec(A\trans) = a_{l, k}$, where $a_{l, k}$ is the entry of $A$ at index $(l, k)$.
    On the other hand, we compute $A E = (0, \dots, a^{(k)}, \dots, 0) \eqqcolon \bb{A}$, with $a^{(k)}$ signifying the $k$-th column of $A$ located at the $l$-th column of $\bb{A}$.
    Therefore, by applying the trace operator to $A E$, we only select the $l$-th row element from the $l$-th column, which thus implies that $\trace{A E} = a_{l, k}$.

    Lastly, if we swap out $\vecmat$ by $\vecupper$, or $\vecstrict$, and $\matvec$ by $\uppervec$, or $\strictvec$ respectively, only the total dimension of the matrices $A$ and $E$, and the indexing from $e_i$ to the tuple $(k, l)$ change slightly.
    Hence, both of the remaining cases follow analogously.

    \itodo{mention that $\uppervec, \strictvec$ can also operate on full matrices by omission of indices}
\end{proof}

\itodo{make sure that the class from which we parametrize is actually consistent. it should suffice to use real numbers instead of complex ones here!}

\begin{theorem}[{Cf.~\cite[Theorem~3.3]{Schwerdtner2023} or~\cite[Lemma~4]{Schwerdtner2021} or~\cite[Theorem~1]{Schwerdtner2022}}]\label{lem:inner-sobmor-gradient}
    Let $\xi \in \bb{C}^{N}$ be a parameter for the parametrization of a port-Hamiltonian system $\Sigma_\msc{ph}(\theta) = (E(\theta), J(\theta), R(\theta), G(\theta), P(\theta), S(\theta), N(\theta))$ with a transfer function $\mcl{H}_\theta$, $\mcl{H}$ be the transfer function of the original system, $s \in \overline{\bb{C}_+}$ be a sample point in the closed right complex halfplane, $\sigma_j \in \bb{C}$ be the $i$-th largest singular value of $\mcl{H}(s) - \mcl{H}_\xi(s)$, and $l, r \in \bb{C}^{n_u}$ be the corresponding left and right singular vectors.
    If the singular value $\sigma_i \neq 0$ is simple, then the function $\theta \mapsto \sigma_i(\mcl{H}(s) - \mcl{H}_\theta(s))$ is differentiable in an open neighbourhood of $\xi$.
    In particular, using the shorthand $\Sigma_\msc{ph}(\xi) = (E, J, R, G, P, S, N)$ we can define the additional terms
    \begin{equation}\label{eq:inner-sobmor-gradient-helpers}
        F \coloneqq (s E - (J - R)),\quad a \coloneqq F\inv (G - P) r,\quad b\herm \coloneqq \ell\herm {(G + P)}\trans F\inv
    \end{equation}
    such that the gradient $\nabla_\theta \sigma_i(\mcl{H}(s) - \mcl{H}_\xi(s)) = {({\dif E}\trans, {\dif J}\trans, {\dif R}\trans, {\dif G}\trans, {\dif P}\trans, {\dif S}\trans, {\dif N}\trans)}\trans$ with respect to $\theta$ is made up of the components
    \begin{equation}\label{eq:inner-sobmor-gradient-components}
        \begin{aligned}
            \dif E &= \re{\uppervec\left( s \vecupper(\theta_\msc{E}) (a b\herm + {(a b\herm)}\trans) \right)}, \\
            \dif J &= - \re{\strictvec(a b\herm - {(a b\herm)}\trans)}, \\
            \dif R &= \re{\uppervec\left( \vecupper(\theta_\msc{R}) (a b\herm + {(a b\herm)}\trans) \right)}, \\
            \dif G &= - \re{\matvec(a \ell\herm + {(r b\herm)}\trans)}, \\
            \dif P &= - \re{\matvec(a \ell\herm - {(r b\herm)}\trans)}, \\
            \dif S &= - \re{\uppervec\left( \vecupper(\theta_S) (r \ell\herm + {(r \ell\herm)}\trans) \right)}, \text{ and} \\
            \dif N &= \re{\strictvec(r \ell\herm - {(r \ell\herm)}\trans)}.
        \end{aligned}
    \end{equation}

    \itodo{fix strict triangular matrices! The terms are wrong there!}
    \itodo{improve mention that $Q = \id$}
    \itodo{replace $n_u$ with a consistent denomination for the dimension of the output!}
\end{theorem}

\itodo{make sure that the large parameter dimension is consistently named throughout the thesis!}

\itodo{Question to the reader: Is the following proof too long? Should it be moved to an appendix?}

\begin{proof}
    \itodo{Appendix?; cf.~\cite[Theorem~3.3]{Schwerdtner2023} or~\cite[Lemma~4]{Schwerdtner2021} or~\cite[Theorem~1]{Schwerdtner2022}}
    \itodo{this proof needs~\cite{Mehrmann2005} for the differentiability!}

    We start of by showing the statement for $\dif E$.
    Firstly, we make an important argument which directly carries over to the gradient components of $J, R$, and potentially $Q$.
    Let $\varepsilon > 0$ be given and consider the parameter $\theta_E + \varepsilon e_i$, which has been updated at the $i$-th relevant component.
    By abuse of notation we directly carry this notation over to $\theta + \varepsilon e_i$ by padding $e_i$ accordingly with zeros such that the affected component in the $E$ matrix remains the same.
    By applying the new parameter to the port-Hamiltonian system $\Sigma_\msc{ph}(\theta + \varepsilon e_i)$, the $E$ matrix update thus is
    \begin{equation}\label{eq:sobmor-e-matrix-update}
        \begin{aligned}
            &{\vecupper(\theta_E + \varepsilon e_i)}\trans \vecupper(\theta_E + \varepsilon e_i) \\
             &= {\vecupper(\theta_E)}\trans \vecupper(\theta_E) + \varepsilon \left( {\vecupper(e_i)}\trans \vecupper(\theta_E) + {\vecupper(\theta_E)}\trans \vecupper(e_i) \right) + \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i) \\
             &= E(\theta) + \varepsilon \left( {\vecupper(e_i)}\trans \vecupper(\theta_E) + {\vecupper(\theta_E)}\trans \vecupper(e_i) \right) + \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i).
        \end{aligned}
    \end{equation}
    Because we want to take the first derivative of~\eqref{eq:sobmor-e-matrix-update} w\@. r\@. t\@. $\varepsilon$, we can neglect the last summand and define $\Delta_E \coloneqq {\vecupper(e_i)}\trans \vecupper(\theta_E) + {\vecupper(\theta_E)}\trans \vecupper(e_i)$.
    Afterwards, we compute
    \begin{equation}\label{eq:sobmor-e-variation}
        \begin{aligned}
            \mcl{H}_{\theta + \varepsilon e_i}(s) &= {(G + P)}\trans Q {\left( s (E + \varepsilon \Delta_E + \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i)) - (J - R) Q \right)}\inv (G - P) + (S - N) \\
             &= {(G + P)}\trans Q \underbrace{{\left( F + s \varepsilon \Delta_E + s \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i) \right)}\inv}_{\eqqcolon (*)} (G - P) + (S - N)
        \end{aligned}
    \end{equation}
    To compute $(*)$, we use the fact that if some matrix $M$ satisfies $\norm{M}{} < 1$, then ${(\id + M)}\inv = \sum\limits_{m = 0}^\infty {(-M)}^m$.
    We use the regularity of the matrix stencil $F$ and calculate
    \begin{equation}\label{eq:sobmor-e-stencil-inversion}
        \begin{alignedat}{3}
            &(*) &= &{\left( F (\id + s \varepsilon F\inv \Delta_E + s \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i)) \right)}\inv \\
            & &= &{\left( \id + s \varepsilon F\inv \Delta_E + s \varepsilon^2 F\inv {\vecupper(e_i)}\trans \vecupper(e_i) \right)}\inv F\inv \\
            & &= &\sum\limits_{m = 0}^\infty {\left( - s \varepsilon F\inv \Delta_E - s \varepsilon^2 F\inv {\vecupper(e_i)}\trans \vecupper(e_i) \right)}^m F\inv.
        \end{alignedat}
    \end{equation}
    Moreover, we can express the Taylor series expansion of the parametrized transfer function $\mcl{H}_\theta$ in terms of~\eqref{eq:sobmor-e-stencil-inversion}, resulting in
    \begin{equation}\label{eq:sobmor-e-taylor}
        \mcl{H}_{\theta + \varepsilon e_i}(s) = \mcl{H}_\theta(s) - {(G + P)}\trans Q \sum\limits_{m = 1}^\infty {\left( s \varepsilon F\inv \Delta_E + s \varepsilon^2 F\inv {\vecupper(e_i)}\trans \vecupper(e_i) \right)}^m F\inv (G - P).
    \end{equation}
    As argued in~\cite[Theorem~1]{Schwerdtner2022}, the map $\varepsilon \mapsto \sigma_i\left( \mcl{H}(s) - \mcl{H}_{\theta + \varepsilon e_i}(s) \right)$ is differentiable for small enough $\varepsilon > 0$ as a consequence of~\cite{Lancaster1964}, thusly admitting the following way of expressing the gradient component
    \begin{equation}\label{eq:sobmor-e-trace-form}
        \begin{aligned}
            &\res{\frac{\dif}{\dif \varepsilon} \sigma_i(\mcl{H}(s) - \mcl{H}_{\theta + \varepsilon e_i}(s))}{\varepsilon = 0} = \re{s \ell\herm {(G + P)}\trans Q F\inv \Delta_E F\inv (G - P) r} \\
             &\underbrace{=}_{(\msc{A})} \re{\trace{s F\inv (G - P) r \ell\herm {(G + P)}\trans Q F\inv \Delta_E}} \\
             &= \re{\trace{s a b\herm \left( {\vecupper(e_i)}\trans \vecupper(\theta_E) + {\vecupper(\theta_E)}\trans \vecupper(e_i) \right)}} \\
             &\underbrace{=}_{(\msc{B})} \re{\trace{\left( {(a b\herm)}\trans {\vecupper(\theta_E)}\trans + a b\herm {\vecupper(\theta_E)}\trans \right) \vecupper(e_i)}} \\
             &\underbrace{=}_{(\msc{C})} \re{s \uppervec\left( \vecupper(\theta_E) \bigl( a b\herm + {(a b\herm)}\trans \bigr) \right)}.
        \end{aligned}
    \end{equation}
    In this transformation, we have applied the following arguments:
    \begin{enumerate}[label= (\scshape{\alph*}):]
        \item For a vector product $u\herm v$ it is equivalent to compute $\trace{v u\herm}$,
        \item for the first summand of $\Delta_E$ we exploit that the trace operator is cyclic and invariant under transpositions, resulting in $\trace{A B C\trans D} = \trace{D\trans C B\trans A\trans} = \trace{{(A B)}\trans D\trans C}$, and
        \item Lemma~\ref{lem:v2m-function-trace}.
    \end{enumerate}
    The other matrices contained in the definition of $F$ can be computed in a similar way, with slight changes in the variational terms corresponding to $\Delta_E$.
    As an example, we compute the final steps of the equivalent of~\eqref{eq:sobmor-e-trace-form} for $J$ with $\Delta_J = {\vecstrict(e_i)}\trans - \vecstrict(e_i)$
    \begin{equation*}
        \begin{aligned}
            &\res{\frac{\dif}{\dif \varepsilon} \sigma_i(\mcl{H}(s) - \mcl{H}_{\theta + \varepsilon e_i}(s))}{\varepsilon = 0} = - \re{\trace{a b\herm \Delta_J}} = - \re{\trace{a b\herm \left({\vecstrict(e_i)}\trans - \vecstrict(e_i)\right)}} \\
             &\underbrace{=}_{(\msc{b})} - \re{\trace{\left( {(a b\herm)}\trans - a b\herm \right) \vecstrict(e_i)}} \underbrace{=}_{(\msc{c})} - \re{\strictvec\left( a b\herm - {(a b\herm)}\trans \right)}.
        \end{aligned}
    \end{equation*}

    For the matrices $G$ and $P$ we can follow an easier procedure.
    As before we compute the matrix $G$ for the updated parameter $\theta_G + \varepsilon e_i$, and define $\Delta_G \coloneqq \vecmat(e_i)$.
    Similarly to~\eqref{eq:sobmor-e-variation} we calculate the updated transfer function to be
    \begin{equation*}
        \begin{aligned}
            \mcl{H}_{\theta + \varepsilon e_i}(s) &= {(G + \varepsilon \Delta_G + P)}\trans Q F\inv (G + \varepsilon \Delta_G - P) + (S - N) \\
             &= \mcl{H}_\theta(s) + \varepsilon {\Delta_G}\trans Q F\inv (G - P) + \varepsilon {(G + P)}\trans Q F\inv \Delta_G + \varepsilon^2 {\Delta_G}\trans Q F\inv \Delta_G.
        \end{aligned}
    \end{equation*}
    By considering the Taylor expansion of $\mcl{H}_{\theta + \varepsilon e_i}$ as in~\eqref{eq:sobmor-e-taylor}, we can observe that the derivative of the singular value $\sigma_i(\mcl{H}(s) - \mcl{H}_{\theta + \varepsilon e_i}(s))$ equals
    \begin{equation*}
        \begin{aligned}
            &\res{\frac{\dif}{\dif \varepsilon} \sigma_i(\mcl{H}(s) - \mcl{H}_{\theta + \varepsilon e_i}(s))}{\varepsilon = 0} = - \re{\ell\herm \left( {\Delta_G}\trans Q F\inv (G - P) + {(G + P)}\trans Q F\inv \Delta_G \right) r} \\
             &\underbrace{=}_{(\msc{a})} - \re{\trace{Q F\inv (G - P) r \ell\herm {\Delta_G}\trans + r \ell\herm {(G + P)}\trans Q F\inv \Delta_G}} \\
             &= - \re{\trace{a \ell\herm {\Delta_G}\trans + r b\herm \Delta_G}} \underbrace{=}_{(\msc{b})} - \re{\trace{\left( {(a \ell\herm)}\trans + r b\herm \right) \Delta_G}} \\
             &= -\re{\trace{\left( {(a \ell\herm)}\trans + r b\herm \right) \vecmat(e_i)}} \underbrace{=}_{(\msc{c})} -\re{\matvec\left( a \ell\herm + {(r b\herm)}\trans \right)}.
        \end{aligned}
    \end{equation*}
    The result for $P$ can be computed analogously with a change of signs.

    Finally, the computation for the matrices $S$ and $N$ is a combination of the two previously described procedures.
    With $\Delta_S \coloneqq {\vecupper(e_i)}\trans \vecupper(\theta_S) + {\vecupper(\Delta_S)}\trans \vecupper(e_i)$ we can repeat~\eqref{eq:sobmor-e-trace-form} for $S$
    \begin{equation*}
        \begin{aligned}
            &\res{\frac{\dif}{\dif \varepsilon} \sigma_i(\mcl{H}(s) - \mcl{H}_{\theta + \varepsilon e_i}(s))}{\varepsilon = 0} = - \re{\trace{r l\herm \Delta_S}} \\
             &= - \re{\trace{\left( {(r l\herm)}\trans + r l\herm \right) {\vecupper(\theta_S)}\trans \vecupper(e_i)}} \\
             &= - \re{\uppervec\left( \vecupper(\theta_S) \left( r l\herm + {(r l\herm)}\trans \right) \right)}.
        \end{aligned}
    \end{equation*}
    Again, these equalities can be applied to $N$ in an analogous manner to prove the theorem.

    \itodo{remove $Q$ or change the definitions to use $c$ vector as well!}
    \itodo{find statement that if singular values are simple, then the stencil is invertible (though this may already result from the assumptions we have made on the system matrices\dots)}
\end{proof}

\begin{remark}
    We note that we intentionally choose the port-Hamiltonian form in such a way that $Q = \id$.
    Without this assumption, the gradient components become more complicated because one would have to adjust some of the $a$ terms in~\eqref{eq:inner-sobmor-gradient-components}.
\end{remark}

With the gradient of the objective functional in hand, the authors in~\cite{Schwerdtner2023} use an inner BFGS iteration for each value of $\gamma$ they optimize for.
They terminate the outer iteration as soon as the objective $\mcl{L}$ is greater than zero, because at that point the $\mcl{H}_\infty$ norm definitely has to be larger than or equal to $\gamma$, cf.~\cite[Section~3.2.2]{Schwerdtner2023}.
The selection processes of both the sample points $\mcl{S}$ and the thresholds $\gamma$ can be varied.
Among some of the options for the sample points are $\iu \omega \in \iu \bb{R}$, where $\omega$ are logarithmically spaced points on the real axis, or an adaptive sample adjusted for the value of the objective function between two sample points, cf.~\cite{SV2021}.

% parametrization of ph system and analytical gradients of maximum singular value difference~\cite{Schwerdtner2021}; default loss, not yet ``leveled least squares'', includes noisy errors
% parametrization, no gradients but mentions results from other papers with analyticald grads, error adaptive selection of sampling points~\cite{SV2021}; levelled loss with maximum singular value
% parametrization, analytical gradients, remarks on asymptotic stability~\cite{Schwerdtner2022}; levelled loss with multiple singular values
% parametrization, analytical gradients, remarks on initialization strategy~\cite{Schwerdtner2023}; levelled loss with multiple singular values
