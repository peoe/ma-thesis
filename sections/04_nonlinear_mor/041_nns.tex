\section{MOR with \acp{NN}}\label{sec:nn-mor}

This section relies on a basic understanding of \ac{NN} structure.
For basic references on \acp{NN} and Deep Learning, refer to sources such as~\cite{Goodfellow2016, Kubat2017, Sarker2021} as well as the other resources therein.

To approximate a function $f \colon \bb{R}^n \mapsto \bb{R}$ or more generally an operator $A \colon \bb{R}^n \mapsto \bb{R}^m$ with \acp{NN}, the general procedure consists of first constructing a \emph{parametrized network} $A_\theta$ intended to approximate the operator $A$, training the network by successive updates to the parameter $\theta$ obtained by evaluating an approximate gradient of a \emph{loss function} which ressembles some measured error between the original operator $A$ and its computed counterpart $A_\theta$.
By the choice of nonlinear \emph{activation functions} in the network's hidden layers this method can create very accurate nonlinear models while being extremely parallelizable, however it nevertheless requires both large amounts of data and appropriate design choices to generalize from restricted input data to actual applications.

Besides the ability to mimic operator behaviour, \ac{NN} architectures can also produce results similar to reduced models in linear MOR.
To this end, modern Deep Learning techniques are employed to inherently produce a low order network.
In recent publications such as~\cite{Lee2020, Salvador2021, Benner2022, Kim2022, Buchfink2023}, \emph{Deep Convolutional Autoencoders} (DCAEs) are introduced.
These DCAEs consist of two parametrized components: an encoder $e_\theta \colon \bb{R}^n \mapsto \bb{R}^r$ and a decoder $d_\theta \colon \bb{R}^r \mapsto \bb{R}^n$.
Here, the encoder serves as a means to reduce the state order, and the decoder acts as the reconstruction of the reduced state back to the full order.
To train the encoder-decoder pair, we typically choose a loss function that corresponds to the projection error over a sample set $\mcl{S}$ of full order states
\begin{equation}\label{eq:reduction-loss}
    \mcl{L}_{\msc{data}}(\theta) \coloneqq \frac{1}{\abs{\mcl{S}}{}} \sum\limits_{x \in \mcl{S}} \norm{x - d\left( e(x) \right)}{}^2.
\end{equation}
In addition to this loss function we can also express systemic constraints on the reduced order models.
As an example, the approach highlighted in~\cite{Buchfink2023} introduces a second loss function $\mcl{L}_{\msc{sympl}}(\theta)$ such that the combined loss function $\mcl{L} \coloneqq \alpha \mcl{L}_{\msc{data}} + (1 - \alpha) \mcl{L}_{\msc{sympl}}$ not only generates a reduced order model but also weakly constraints this reduced order model to exhibit symplectic structure very much similar to the restrictions we enforced upon \acp{LTI} to conform to the \ac{PH} view.

When we consider \ac{NN}-based approximation we get a clear distinction between the offline and the online phases: We have to perform expensive calculations while training the model offline, and thereafter we can efficiently evaluate the simple structure of the network to quickly get results in the online phase.
Unfortunately, the training process conceptually obfuscates how the low order state representation $e(x)$ is connected to the full order state $x$.
This results in sophisticated nonlinear models that are entirely \emph{unexplainable}.
We aim to circumnavigate the unexplainability introduced in the \ac{NN} setting which means that we have to consider methods which are directly linked to the systems.
In our case we specifically want to avoid direct access to the underlying system matrices because these are not going to be available in the framework we want to establish in Section~\ref{sec:mor-quadratically-embedded-manifolds}.
In particular, we only have access to system data consisting of input and output measurements, as well as additional state observations for some situations.
