\section{Optimization-Based Inference}\label{sec:optimization-based-inference}

This subsections deals with optimization-based \ac{PH} \ac{OI} methods.
In essence, all of these methods choose some parametrization of the underlying system matrices and then run an optimization algorithm to minimize the cost functional that enforces \ac{PH} structure.
The choice of algorithm, the construction of the individual optimization objectives, and the inclusion of structural constraints reflecting~\eqref{eq:ph-matrix-structure} heavily varies between publications.
Importantly, we have to consider the initialization of the matrices.
Most of the algorithms are very susceptible to initial value changes because the respective optimization problems are generally hard to solve.

The first method we consider is the nearest \ac{PH} system mentioned at the end of the previous subsection as described in~\cite{Gillis2018} and~\cite{Cherifi2019}.
The main minimization problem is given by the relation to the inferred \ac{LTI} system $\Sigma_\msc{lti}$ with the realization $(A, B, C, D, E)$
\begin{equation}\label{eq:nearest-ph-system-minimization-problem}
    \begin{alignedat}{4}
        \eqinf\limits_{F, J, R, G, P, S, N} &\mcl{J}(F, J, R, G, P, S, N) \coloneqq \norm{A - (J - R)}{F}^2 + \norm{B - (G - P)}{F}^2 \\
        &+ \norm*{C - {(G + P)}\trans}{F}^2 + \norm{D - (S - N)}{F}^2 + \norm{E - F}{F}^2, \\
        &\suchthat -J = J\trans, -N = N\trans, R = R\trans \succcurlyeq 0, S = S\trans \succcurlyeq 0, E = E\trans \succcurlyeq 0, \begin{pmatrix}
            R & P \\
            P\trans & S
        \end{pmatrix} \succcurlyeq 0.
    \end{alignedat}
\end{equation}
In~\cite{Gillis2018}, this problem is solved with the \ac{FGM} in combination with a restarting mechanism to stabilize the convergence of the algorithm.
Along with these features, the objective function~\eqref{eq:nearest-ph-system-minimization-problem} allows for weights on each individual summand prioritizing certain system terms during the optimization.
Besides this advantage, we can computate the cost function's gradient in the Frobenius norm by
\begin{equation*}
    \nabla_X \norm{A X - B}{F}^2 = 2 A\trans (A X - B).
\end{equation*}
As far as initialization is concerned, the authors of~\cite{Gillis2018} propose two methods: initialize with the additional assumption $P = 0$ such that the optimal solution can be given analytically, or solve a \ac{LMI} to compute an initial \ac{PH} realization along the same line of arguments as previous links between \acp{LMI} and positive real systems have done.

Another \ac{FGM}-related method is the \ac{PHDMD} algorithm given in~\cite{Morandin2023}.
As the name implies, this is closely linked to the \ac{DMD} method in that we want to minimize
\begin{equation}\label{eq:ph-dmd-minimization-problem}
    \min \norm*{\begin{pmatrix}
        E \dot{X} \\
        -Y
    \end{pmatrix} - (\mcl{J} - \mcl{R}) \begin{pmatrix}
        X \\
        U
    \end{pmatrix}}{F}^2,\quad \suchthat \mcl{J} = - \mcl{J}\trans, \mcl{R} = \mcl{R}\trans \succcurlyeq 0
\end{equation}
where $X, \dot{X} \in \bb{R}^{n \times k}$ and $U, Y \in \bb{R}^{m \times k}$ are data matrices computed from a sequence of snapshots ${(x_i, u_i, y_i)}_{i = 0}^{k + 1}$ with fixed step size $\dif t > 0$ by calculating
\begin{equation}\label{eq:ph-dmd-discrete-data}
    \begin{alignedat}{3}
        X &\coloneqq \frac{1}{2} {\left( x_{i + 1} + x_i \right)}_{i = 0}^k,\quad &\dot{X} &\coloneqq \frac{1}{\dif t} {\left( x_{i + 1} - x_i \right)}_{i = 0}^k \\
        U &\coloneqq \frac{1}{2} {\left( u_{i + 1} + u_i \right)}_{i = 0}^k,\quad &Y &\coloneqq \frac{1}{2} {\left( y_{i + 1} + y_i \right)}_{i = 0}^k.
    \end{alignedat}
\end{equation}
This method distinguishes itself from the previous \ac{FGM} by the fact that \ac{PHDMD} does not allow restarting within iterations.
Furthermore, the \ac{PHDMD} algorithm does not offer the ability of optimizing the $E$ matrix because in this setting it would lead to the trivial minimizers $\mcl{J} = \mcl{R} = 0$ as shown in~\cite[Remark~3.3]{Morandin2023}.
Another downside of the \ac{PHDMD} algorithm is the fact that the entire procedure is based on a modified implicit midpoint time stepping scheme: instead of utilizing the proper midpoint control value in~\eqref{eq:ph-dmd-discrete-data}, we approximate it via the trapezoidal rule $u\left( t + \frac{1}{2} \dif t \right) \approx \frac{1}{2} \left( u(t) + u(t + \dif t) \right)$.
Effectively, this codifies a restriction of the acceptable time steppers for the \ac{PHDMD} algorithm because using other time stepping methods such as Runge--Kutta does not result in the same accuracy as the implicit midpoint rule, as shown in~\cite[Section~4.1]{Morandin2023}.
However, despite these drawbacks the \ac{PHDMD} algorithm is very practical for tackling the central problem of this thesis because we can easily adapt the problem formulation~\eqref{eq:ph-dmd-minimization-problem} to include higher order data terms.

In order to derive a solution algorithm for the \ac{PHDMD} problem~\eqref{eq:ph-dmd-minimization-problem} we have to consider two steps into consideration:
How do we compute an initial guess of the matrices $\mcl{J}$ and $\mcl{R}$?
How can we successively update the initial guess to improve the matrices?
In~\cite{Morandin2023}, the authors calculate the initialization of $\mcl{J}$ and $\mcl{R}$ using a weighted version of problem~\eqref{eq:ph-dmd-minimization-problem}.
For ease of notation in further equations we define the combined data matrices
\begin{equation}\label{eq:ph-dmd-data-matrices}
    \mcl{Z} \coloneqq \begin{pmatrix}
        E \dot{X} \\
        -Y
    \end{pmatrix} \text{ and } \mcl{T} \coloneqq \begin{pmatrix}
        X \\
        U
    \end{pmatrix} \in \bb{R}^{(n + m) \times k}.
\end{equation}
To define the weighted problem we follow the intention of weighting the information contained within the data matrices $\mcl{Z}$ and $\mcl{T}$.
Multiplying the inner terms in Equation~\eqref{eq:ph-dmd-minimization-problem} from the left by $\mcl{T}\trans$, we calculate the weighted \ac{PHDMD} optimization problem
\begin{equation}\label{eq:weighted-ph-dmd}
    \min \norm*{\mcl{T}\trans \left( \mcl{Z} - (\mcl{J} - \mcl{R}) \mcl{T} \right)}{F}^2,\quad \suchthat \mcl{J} = - \mcl{J}\trans, \mcl{R} = \mcl{R}\trans \succcurlyeq 0.
\end{equation}
The benefit of this formulation is immediate by the next result, and its effects on the solutions of the unweighted \ac{PHDMD} problem~\eqref{eq:ph-dmd-minimization-problem} by the one thereafter.

\begin{lemma}[{Adapted from~\cite[Theorem~3.7]{Morandin2023}}]\label{lem:weighted-ph-dmd-solution}
    Let $\mcl{T}, \mcl{Z} \in \bb{R}^{(n + m) \times k}$ be the two data matrices as defined in Equation~\eqref{eq:ph-dmd-data-matrices} and suppose that $\mcl{T}$ admits the skinny singular value decomposition
    \begin{equation*}
        \mcl{T} = V \Sigma W_1\trans,\quad V \in \bb{R}^{(n + m) \times r},\quad \Sigma \in \bb{R}^{r \times r},\quad W_1 \in \bb{R}^{k \times r},
    \end{equation*}
    where $r = \rank{\mcl{T}}$, $\Sigma$ is regular, and $V$ as well as $W_1$ satisfy $V\trans V = \id[r], W_1\trans W_1 = \id[r]$, respectively.
    Further let $V_2 \in \bb{R}^{k \times (k - r)}$ extend $V_1$ such that $V = \begin{pmatrix}
        V_1 & V_2
    \end{pmatrix}$ satisfies $V\trans V = \id[k]$.
    Then the matrices
    \begin{equation*}
        \mcl{J}^\ast \coloneqq W \Sigma\inv \sksym{\Sigma W\trans \mcl{Z} V_1} \Sigma\inv W\trans,\quad \mcl{R}^\ast \coloneqq W \Sigma\inv \psd{- \Sigma W\trans \mcl{Z} V_1} \Sigma\inv W\trans
    \end{equation*}
    are the unique minimimizers of the weighted \ac{PHDMD} problem~\eqref{eq:weighted-ph-dmd} satisfying
    \begin{equation*}
        \norm*{\mcl{T}\trans \left( \mcl{Z} - (\mcl{J}^\ast - \mcl{R}^\ast) \mcl{T} \right)}{F}^2 = \norm*{\Sigma W\trans \mcl{Z} V_2}{F}^2 + \norm*{\Lambda_+(\Sigma W\trans \mcl{Z} V_1)}{F}^2,
    \end{equation*}
    where for a quadratic matrix $A$ we denote its skew-symmetric projection as $\sksym{A}$, its symmetric positive semidefinite projection as $\psd{A}$, and the diagonal matrix made up of the positive eigenvalues of its symmetric part as $\Lambda_+(A)$.
\end{lemma}

\begin{proof}
    For a detailed proof, see~\cite[Proof of Theorem~3.7 and Lemma~3.10]{Morandin2023}.
\end{proof}

\begin{lemma}[{Adapted from~\cite[Theorem~3.11]{Morandin2023}}]\label{lem:relation-ph-dmd-problems}
    Let $\mcl{T}, \mcl{Z} \in \bb{R}^{(n + m) \times k}$ be the two data matrices as defined in Equation~\eqref{eq:ph-dmd-data-matrices} with $\rank{\mcl{T}} = n + m$.
    Then there exists a constant $c > 0$ such that for all matrices $\mcl{J}, \mcl{R} \in \bb{R}^{(n + m) \times (n + m)}$ it holds that
    \begin{equation*}
        \norm*{\mcl{Z} - (\mcl{J} - \mcl{R}) \mcl{T}}{F}^2 \leq c^2 \norm*{\mcl{T}\trans \left( \mcl{Z} - (\mcl{J} - \mcl{R}) \mcl{T} \right)}{F}^2.
    \end{equation*}
\end{lemma}

\begin{proof}
    Consider the singular value decomposition $\mcl{T} = V \Sigma W\trans$ and define the pseudoinverse of $\mcl{T}$ in terms of its SVD by calculating $\mcl{T}^\dagger \coloneqq W \Sigma^\dagger V\trans$.
    We use the fact that $\rank{\mcl{T}} = n + m$ and therefore observe $\Sigma^\dagger \Sigma = \id[n + m]$.
    Next we use that the matrices $V$ and $W$ are unitary and calculate the relation
    \begin{equation*}
        \norm*{\mcl{Z} - ( \mcl{J} - \mcl{R}) \mcl{T}}{F}^2 = \norm*{{\left( \mcl{T}\trans \right)}^\dagger \mcl{T}\trans \left( \mcl{Z} - ( \mcl{J} - \mcl{R}) \mcl{T} \right)}{F}^2 \leq \norm*{{\left( \mcl{T}\trans \right)}^\dagger}{F}^2 \norm*{\mcl{T}\trans \left( \mcl{Z} - ( \mcl{J} - \mcl{R}) \mcl{T} \right)}{F}^2,
    \end{equation*}
    which concludes the proof after setting $c^2 \coloneqq \norm*{{\left( \mcl{T}\trans \right)}^\dagger}{F}^2$.
\end{proof}

Next we use our current matrices $\mcl{J}$ and $\mcl{R}$ to construct updated versions of both.
For this it is imperative to notice that once $\mcl{J}$ or $\mcl{R}$ are given, we can much more easily compute the remaining matrices when compared to the original \ac{PHDMD} problem~\eqref{eq:ph-dmd-minimization-problem} where we need to solve for both at the same time.
To demonstrate the effect of $\mcl{R}$ being given before hand we consider the original optimization problem~\eqref{eq:ph-dmd-minimization-problem} in terms of the data matrices $\mcl{T}$ and $\mcl{Z}$ defined in~\eqref{eq:ph-dmd-data-matrices}.
We simplify this problem by denoting the fixed contribution of $\mcl{R}$ as $\mcl{Q} \coloneqq \mcl{Z} + \mcl{R} \mcl{T}$ and inserting it into the problem formulation
\begin{equation}\label{eq:ph-dmd-skew-sym-subproblem}
    \min \norm*{\mcl{Z} - (\mcl{J} - \mcl{R}) \mcl{T}}{F}^2 = \min \norm*{\mcl{Q} - \mcl{J} \mcl{T}}{F}^2,\quad \suchthat \mcl{J} = - \mcl{J}\trans;
\end{equation}
the case for a given $\mcl{J}$ follows analogously.
Solutions to the subproblem~\eqref{eq:ph-dmd-skew-sym-subproblem} are given by the following result.

\begin{theorem}
    Let $\mcl{T}, \mcl{Z} \in \bb{R}^{(n + m) \times k}$ be the two data matrices as defined in Equation~\eqref{eq:ph-dmd-data-matrices}, and let $\mcl{T} = V \Sigma W\trans$ denote the singular value decomposition with $r = \rank{\mcl{T}}$ and
    \begin{equation*}
        \Sigma = \begin{pmatrix}
            \Sigma_1 & 0 \\
            0 & 0
        \end{pmatrix},\quad \Sigma_1 = \diag{\sigma_1, \dots, \sigma_r} \in \bb{R}^{r \times r}.
    \end{equation*}
    Next, we define the matrices $\Phi = {(\varphi_{i, j})}_{i, j = 1}^r \in \bb{R}^{r \times r}$, where $\varphi_{i, j} = \frac{1}{\sigma_i^2 + \sigma_j^2}$, and
    \begin{equation*}
        A = \begin{pmatrix}
            \id[r] & 0
        \end{pmatrix} V \mcl{Z} W\trans \begin{pmatrix}
            \id[r] \\
            0
        \end{pmatrix} \in \bb{R}^{r \times r},\quad B = \begin{pmatrix}
            0 & \id[n + m - r]
        \end{pmatrix} V \mcl{Z} W\trans \begin{pmatrix}
            \id[r] \\
            0
        \end{pmatrix} \in \bb{R}^{(n + m - r) \times r},
    \end{equation*}
    with which we further calculate $J_1 = 2 \Phi \odot \sksym{A \Sigma_1}, J_2 = B \Sigma_1\inv$, where the matrix operator $\odot$ denotes the usual Hadamard product and $\sksym{\cdot}$ is the projection onto the set of skew-symmetric matrices as mentioned in Lemma~\ref{lem:weighted-ph-dmd-solution}.
    Then for any skew-symmetric matrix $J_4 \in \bb{R}^{(n + m - r) \times (n + m - r)}$, the matrix
    \begin{equation*}
        \mcl{J} \coloneqq V\trans \begin{pmatrix}
            J_1 & -J_2\trans \\
            J_2 & J_4
        \end{pmatrix}
    \end{equation*}
    is a solution of the subproblem~\eqref{eq:ph-dmd-skew-sym-subproblem}.
    Further, the solution $\mcl{J}$ of~\eqref{eq:ph-dmd-skew-sym-subproblem} is unique if and only if the condition $\rank{\mcl{T}} = n + m$ is satisfied.
\end{theorem}

\begin{proof}
    Consider~\cite[Lemma~2.1]{Deng2003}.
\end{proof}

To solve these individual optimization problems, we have to consider the direct method for the case that $\mcl{R}$ is given, see~\cite[Theorem~3.4]{Morandin2023}, and the algorithmic solution via an \ac{FGM} if we already computed $\mcl{J}$.
Continuing this line of thought we can formulate an iterative algorithm that alternatingly updates $\mcl{J}$ and $\mcl{R}$ using these solution mechanisms.
We show this semianalytical \ac{FGM} in Algorithm~\ref{alg:semianalytical-fgm}.
Notably, this algorithm is very sensitive to initial conditions: if we use an inappropriate initialization, the algorithm is expected to take a long time to complete --- this is a noticeable problem for obtaining quadratically embedded models.

\begin{algorithm}\label{alg:semianalytical-fgm}
    \caption{Semianalytical \ac{FGM} Algorithm, adapted from~\cite[Algorithm~1]{Morandin2023}}
    \KwData{Model data $\mcl{T}, \mcl{Z} \in \bb{R}^{(n + m) \times k}$, initial guess $\mcl{R}_0 = \mcl{R}_0\trans \succcurlyeq 0$}
    $L \coloneqq \sigma_1^2(\mcl{T}),\quad \ell \coloneqq \sigma_r^2(\mcl{T}),\quad q \coloneqq \ell / L$\;
    $\alpha_0 \in \interval[open]{0}{1},\quad Q \coloneqq \mcl{R}_0$\;
    $k \coloneqq 0$\;
    \While{not converged}{
        Solve subproblem~\eqref{eq:ph-dmd-skew-sym-subproblem} with $\mcl{R}_0$ to get $\mcl{J}_{k + 1}$\;
        $E \coloneqq \mcl{J}_{k + 1} \mcl{T} - \mcl{Z}$\;
        $\nabla \coloneqq Q \mcl{T} \mcl{T}\trans - E \mcl{T}$\;
        $\mcl{R}_{k + 1} \coloneqq \psd{Q - \frac{1}{L} \nabla}$\;
        $\alpha_{k + 1} \coloneqq \frac{1}{2} \left( q - \alpha_k^2 + \sqrt{{(q - \alpha_k^2)}^2 + 4 \alpha_k^2} \right)$\;
        $\beta_{k + 1} \coloneqq \frac{\alpha_k (1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}$\;
        $Q \coloneqq \mcl{R}_{k + 1} + \beta_{k + 1} \left( \mcl{R}_{k + 1} - \mcl{R}_k \right)$\;
        $k \coloneqq k + 1$\;
    }
    $\mcl{J} \coloneqq \mcl{J}_k,\quad \mcl{R} \coloneqq \mcl{R}_k$\;
\end{algorithm}

Instead of choosing an \ac{LSQ} approach which fits both the output data and the state data, we can also consider a calibration problem
\begin{equation}\label{eq:günther-calibration-objective}
    \mcl{J}(y, \omega) \coloneqq \underbrace{\frac{1}{2} \int\limits_0^T \abs{y(t) - y_\msc{data}(t)}{}^2 \dif t}_{(A)} + \underbrace{\vphantom{\int\limits_0^T}\frac{\lambda}{2} \abs{\omega - \omega_\msc{ref}}{}^2}_{(B)}, \quad \lambda \geq 0,
\end{equation}
where $\omega = (J, R, Q, G, P, x_0)$ is a vectorized representation of the system matrices and the initial data as described in~\cite{Günther2023}.
The authors compute the gradient of the objective functional by considering the error in the optimization functional's two parts $(A)$ and $(B)$ and constructing the corresponding first order optimality conditions with Fréchet derivatives.
Along the way, they derive an adjoint equation from which they obtain a gradient descent algorithm.
The disadvantage of this approach is that we have to solve the \ac{FOM} at every step of the algorithm.
While this may be feasible on small scale or reduced order models, it appears impractical for models of larger scales such as the quadratic models we consider the main problem of this thesis.

Instead of optimizing over, for example, entire symmetric positive (semi-) definite matrices simultaneously, it would also be reasonable to decompose all matrices into distinctive basis elements such that we can adjust the coefficients through a parameter optimization for the individual basis elements, thus allowing for simpler constraints.
To this end, we discuss a few publications that use this idea in multiple, but very similar, ways.

We deconstruct the matrix blocks $\mcl{J}$ and $\mcl{R}$ of a \ac{PH} system in the form
\begin{equation*}
    \begin{pmatrix}
        \dot{x} \\
        y
    \end{pmatrix} = \begin{pmatrix}
        J - R & G - P \\
        (G + P)\trans & S - N
    \end{pmatrix} \begin{pmatrix}
        x \\
        u
    \end{pmatrix}.
\end{equation*}
For such an exemplary skew-symmetric matrix $J \in \bb{R}^{3 \times 3}$ we decompose into the following three basis elements
\begin{equation}\label{eq:skew-symmetric-basis-decomposition}
    J(\omega) = \omega_1 \begin{pmatrix}
        0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix} + \omega_2 \begin{pmatrix}
        0 & 0 & 1 \\
        0 & 0 & 0 \\
        -1 & 0 & 0
    \end{pmatrix} + \omega_3 \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & -1 & 0
    \end{pmatrix}.
\end{equation}
Instead of considering the gradient over the entire matrix, we can now analyze the gradient through the set of parameters $\omega = {(\omega_1, \dots, \omega_p)}\trans \in \bb{R}^{n_J}$ as the vector $\nabla_\omega J(\omega) \in \bb{R}^{n_J}$.
Analogously, in a similar fashion to~\eqref{eq:skew-symmetric-basis-decomposition}, we consider an exemplary positive definite matrix $R \in \bb{R}^{3 \times 3}$ with the basis
\begin{equation}\label{eq:spsd-basis-decomposition}
    R(\omega) = T(\omega) T(\omega)\trans,\quad T(\omega) = \begin{pmatrix}
        \omega_1 & 0 & 0 \\
        \omega_2 & \omega_3 & 0 \\
        \omega_4 & \omega_5 & \omega_6
    \end{pmatrix}.
\end{equation}
Following~\cite{Najnudel2021} we can also parametrize the Hamiltonian $H$, represented solely by $E$ in our case, using a parametrized kernel estimation and calculate the gradient of the error $\norm{\Sigma_\msc{ph}(\theta) - \Sigma_\msc{ph}}{F}^2$ by analytical computations.
The authors then use an Interior Points Method with an additional logarithmic barrier term in the objective function, where the barrier penalizes the matrix coefficients if they do not form a positive definite matrix; hence, this method relies on a weakly enforced constraint similar to the weak symplectic \ac{DCAE} from Section~\ref{sec:nn-mor}.
The biggest downside of this approach is the fact that the error measure requires an explicit reference system $\Sigma_\msc{ph}$, which is why it is not suitable for our problem.

Unlike the approach from~\cite{Najnudel2021}, the authors of~\cite{Schwerdtner2021, SV2021, Schwerdtner2023, SV2023} parametrize the matrices analogously to~\eqref{eq:skew-symmetric-basis-decomposition} and~\eqref{eq:spsd-basis-decomposition}.
To this end, we introduce the following vector-to-matrix functions
\begin{alignat}{4}
    &\vecmat \colon \bb{C}^{n \cdot m} \to \bb{C}^{m \times n},\quad &&\theta \mapsto \begin{pmatrix}
        v_1 & \cdots & v_n \\
        \vdots & \ddots & \vdots \\
        v_{(n - 1) \cdot m + 1} & \cdots & v_{n \cdot m}
    \end{pmatrix}, \label{eq:v2m}\\
    &\vecupper \colon \bb{C}^{\frac{n (n + 1)}{2}} \to \bb{C}^{n \times n},\quad &&\theta \mapsto \begin{pmatrix}
        v_1 & v_2 & \cdots & v_n \\
        0 & v_{n + 1} & \cdots & v_{2n - 1} \\
        0 & 0 & \ddots & \vdots \\
        0 & 0 & 0 & v_{\frac{n (n + 1)}{2}}
    \end{pmatrix}, \label{eq:v2u}\\
    &\vecstrict \colon \bb{C}^{\frac{n (n - 1)}{2}} \to \bb{C}^{n \times n},\quad &&\theta \mapsto \begin{pmatrix}
        0 & v_1 & v_2 & \cdots & v_{n - 1} \\
        0 & 0 & v_n & \cdots & v_{2n - 3} \\
        0 & 0 & 0 & \ddots & \vdots \\
        0 & 0 & 0 & 0 & v_{\frac{n (n - 1)}{2}} \\
        0 & 0 & 0 & 0 & 0
    \end{pmatrix} \label{eq:v2s}
\end{alignat}
and their respective inverse matrix-to-vector functions, operating on general rectangular or quadratic matrices under the assumption that unnecessary matrix entries are simply ignored, given by
\begin{equation}\label{eq:m2v-functions}
    \matvec \colon \bb{C}^{n \times m} \to \bb{C}^{n \cdot m},\quad \uppervec \colon \bb{C}^{n \times n} \to \bb{C}^{\frac{n (n + 1)}{2}}, \text{ and } \strictvec \colon \bb{C}^{n \times n} \to \bb{C}^{\frac{n (n - 1)}{2}}.
\end{equation}
With the definitions~(\ref{eq:v2m}--\ref{eq:m2v-functions}) we can then parametrize the \ac{PH} system's matrices as follows
\begin{equation}\label{eq:sobmor-parametrization}
    \begin{alignedat}{3}
        E(\theta) &\coloneqq {\vecupper(\theta_\msc{E})}\trans \vecupper(\theta_\msc{E}),\quad &Q(\theta) &\coloneqq {\vecupper(\theta_\msc{Q})}\trans \vecupper(\theta_\msc{Q}), \\
        J(\theta) &\coloneqq {\vecstrict(\theta_\msc{J})}\trans - \vecstrict(\theta_\msc{J}),\quad &R(\theta) &\coloneqq {\vecupper(\theta_\msc{R})}\trans \vecupper(\theta_\msc{R}), \\
        N(\theta) &\coloneqq {\vecstrict(\theta_\msc{N})}\trans - \vecstrict(\theta_\msc{N}),\quad &S(\theta) &\coloneqq {\vecupper(\theta_\msc{S})}\trans \vecupper(\theta_\msc{S}), \\
        G(\theta) &\coloneqq \vecmat(\theta_\msc{G}),\quad &P(\theta) &\coloneqq \vecmat(\theta_\msc{P}).
    \end{alignedat}
\end{equation}
In Definition~\eqref{eq:sobmor-parametrization} we have implicitly restricted the overall parameter $\theta \in \bb{R}^N$ to partial parameters such as $\theta_J \in \bb{R}^{n_J}$ corresponding to the individual entries of the system matrices.
Note that the matrices $R, P$, and $S$ can also be amalgated into a single block matrix $W \in \bb{R}^{(n + m) \times (n + m)}$ as in~\cite{Schwerdtner2021}.
The \ac{SOBMOR} algorithm as described in~\cite{SV2023} too uses a data-based objective functional such as the one in Equation~\eqref{eq:günther-calibration-objective} from~\cite{Günther2023}.
In contrast to Equation~\eqref{eq:günther-calibration-objective}, the authors compute the error as a sum of singular values of an error matrix $\tfunc(s_i) - \tfunc_\theta(s_i) \in \bb{R}^{m \times m}$, where $s_i \in \bb{C}$ are sample points, and $\tfunc$ and $\tfunc_\theta$ denote the transfer functions of the \ac{FOM} $\Sigma_\msc{ph}$ as well as the reduced system $\hat{\Sigma}_\msc{ph}$.
This error term directly relates the optimized system to the $\mcl{H}_\infty$ norm from truncation-based model reduction described in Section~\ref{subsec:balanced-truncation}.
The computation of the $\mcl{H}_\infty$ norm, however, is slow in repeated settings as well as for large scale systems, rendering it undesirable as an optimization objective.

For this thesis, we only consider the levelled objective functional on a set of sample points $\mcl{S}$, which includes the $L$ largest singular values
\begin{equation}\label{eq:sobmor-objective}
    \mcl{L}(\gamma, \tfunc, \tfunc_\theta, \mcl{S}) \coloneqq \frac{1}{\gamma} \sum\limits_{s \in \mcl{S}} \sum\limits_{i = 1}^L {\left( {[\sigma_i(\tfunc(s) - \tfunc_\theta(s)) - \gamma]}_+ \right)}^2,
\end{equation}
where we define ${[x]}_+ \coloneqq \max \{ 0, x \}$.
In~\cite{Schwerdtner2021, SV2021}, the authors consider only the $L$ largest singular value for every datum but in the later publications~\cite{Schwerdtner2023, SV2023} they rely on a fixed number of the largest singular values instead.
In practice, the levelled \ac{LSQ} truncates the minimum error at every optimization step, which then allows us to update the bound via either specifying a fixed sequence of tolerances or by adaptively performing a bisection algorithm on the bound.

In order to compute the gradient of the objective function~\eqref{eq:sobmor-objective} with respect to $\theta$, we define $f_i(s; \theta) \coloneqq \sigma_i(\tfunc(s) - \tfunc_\theta(s))$ and $g(\theta) \coloneqq L(\gamma, \tfunc, \tfunc_\theta, \mcl{S})$ to denote the inner and outer function, respectively.
We first calculate the derivative of the outer function $g$ and afterwards the inner derivative of $f$.
For a parameter $\theta \in \bb{R}^N$ we consider the gradient for each of its components $\theta_1, \dots, \theta_N$ at the point $\xi \in \bb{R}^N$ given by $\nabla_\theta g(\xi) = {(\pd[\theta_1]{} g(\xi), \dots, \pd[\theta_N]{} g(\xi))}\trans$.
We compute each of its left and right partial derivatives $\pd[]{-}$ and $\pd[]{+}$, respectively, by evaluating
\begin{equation}\label{eq:outer-sobmor-objective-gradient}
    \begin{alignedat}{1}
        \pd[\theta_l]{} g(\xi) &= \frac{2}{\gamma} \sum\limits_{f_i(s; \xi) > \gamma} (f_i(s; \xi) - \gamma) \pd[\theta_l]{+} f_i(s; \xi) \\
        &= \frac{2}{\gamma} \sum\limits_{f_i(s; \xi) > \gamma} (f_i(s; \xi) - \gamma) \pd[\theta_l]{-} f_i(s; \xi),\quad l = 1, \dots, N.
    \end{alignedat}
\end{equation}
As noted in~\cite[Remark~3.8]{SV2023}, the inner function $f$ is not differentiable at $\xi \in \bb{C}$ in general but the sum of the $L$ singular values still is.
Furthermore, if all singular values represented in $f_i(s; \xi)$ that are greater than $\gamma$ are simple, then the left and right partial derivatives coincide and can be computed explicitly.
Henceforth, we assume that this simplicity condition on the singular values holds true for all the relevant ones.
In order to compute the gradients of the inner functions $f_i$, we require the following Lemma.

\begin{lemma}[{Adapted from~\cite[Lemma~3.4]{SV2023} and~\cite[Lemma~3]{Schwerdtner2023}}]\label{lem:v2m-function-trace}
    Let $A \in \bb{C}^{n \times m}$ be a matrix and $e_i \in \bb{R}^{n \cdot m}$ be the $i$-th standard basis vector.
    Then it holds that
    \begin{equation}\label{eq:v2m-trace-equation}
        \trace{A \vecmat(e_i)} = e_i\trans \matvec(A\trans).
    \end{equation}
    Analogous versions of~\eqref{eq:v2m-trace-equation} hold for the case that we substitute $\vecupper$ or $\vecstrict$ in place of $\vecmat$ and $e_i$ is the $i$-th standard basis vector of appropriate dimension.
    In these cases, $\vecmat$ and $\matvec$ are replaced by $\vecupper$ and $\uppervec$, and $\vecstrict$ and $\strictvec$, respectively.
\end{lemma}

\begin{proof}
    We adapt the proof of a very similar statement from~\cite[Lemma~3.4]{SV2023}.
    We only demonstrate why the statement holds for the functions $\vecmat$ and $\matvec$.
    Let $A \in \bb{C}^{m \times n}$ be a matrix and consider $e_i$ to be the $i$-th standard basis vector of $\bb{R}^{n \cdot m}$.
    Suppose that $i$ corresponds to some index pair $(k, l) \in \bb{N}^2$ such that $\vecmat(e_i) = {(\delta_{i, k} \cdot \delta_{j, l})}_{i, j = 1}^{n, m} \eqqcolon E$.
    By consulting the definition of $\matvec$, we see that
    \begin{equation*}
        \matvec(A\trans) = {(a_{1, 1}, \dots, a_{m, 1}, a_{1, 2}, \dots, a_{m, n})}\trans.
    \end{equation*}
    Thus, by multiplication we calculate $e_i\trans \matvec(A\trans) = a_{l, k}$, where $a_{l, k}$ is the entry of $A$ at index $(l, k)$.
    On the other hand, we compute $A E = (0, \dots, a^{(k)}, \dots, 0) \eqqcolon \bb{A}$, with $a^{(k)}$ signifying the $k$-th column of $A$ located at the $l$-th column of $\bb{A}$.
    Therefore, by applying the trace operator to $A E$, we only select the $l$-th element from the $l$-th column of $\bb{A}$, corresponding to the $l$-th element of the $k$-th column of $A$, which thus implies that $\trace{A E} = a_{l, k}$.

    Lastly, if we substitute $\vecmat$ by $\vecupper$ or $\vecstrict$, and $\matvec$ by $\uppervec$ or $\strictvec$, respectively, only the total dimension of the matrices $A$ and $E$ and the indexing from $e_i$ to the tuple $(k, l)$ change slightly.
    Hence, both of the remaining cases follow analogously.
\end{proof}

\begin{theorem}[{Adapted from~\cite[Theorem~3.3]{SV2023},~\cite[Lemma~4]{Schwerdtner2021} and~\cite[Theorem~1]{Schwerdtner2023}}]\label{lem:inner-sobmor-gradient}
    Let $\theta \in \bb{R}^{N}$ be a parameter for the parametrization of the \ac{PH} realization $(E(\theta), J(\theta), R(\theta), G(\theta), P(\theta), S(\theta), N(\theta))$ with $\tfunc_\theta$ the corresponding parametrized transfer function, $\tfunc$ be the transfer function of the original system, $s \in \overline{\bb{C}_+}$ be a sample point in the closed right complex halfplane, $\sigma_j \in \bb{C}$ be the $j$-th largest singular value of $\tfunc(s) - \tfunc_\theta(s)$, and $l, r \in \bb{C}^m$ be the left and right singular vectors corresponding to $\sigma_j$.
    If the singular value $\sigma_i \neq 0$ is simple, then the function $\theta \mapsto \sigma_i(\tfunc(s) - \tfunc_\theta(s))$ is differentiable in an open neighbourhood of $\theta$.
    In particular, using the shorthand $\Sigma_\msc{ph}(\theta) = (E, J, R, G, P, S, N)$, we can define the additional terms
    \begin{equation}\label{eq:inner-sobmor-gradient-helpers}
        F \coloneqq (s E - (J - R)),\quad a \coloneqq F\inv (G - P) r,\quad b\herm \coloneqq \ell\herm {(G + P)}\trans F\inv
    \end{equation}
    such that the gradient $\nabla_\theta \sigma_i(\tfunc(s) - \tfunc_\theta(s)) = {({dE}\trans, {dJ}\trans, {dR}\trans, {dG}\trans, {dP}\trans, {dS}\trans, {dN}\trans)}\trans \in \bb{R}^N$ with respect to $\theta$ is made up of the consituent vectors
    \begin{equation}\label{eq:inner-sobmor-gradient-components}
        \begin{aligned}
            dE &= \re{\uppervec\left( s \vecupper(\theta_\msc{E}) (a b\herm + {(a b\herm)}\trans) \right)}, \\
            dR &= \re{\uppervec\left( \vecupper(\theta_\msc{R}) (a b\herm + {(a b\herm)}\trans) \right)}, \\
            dS &= - \re{\uppervec\left( \vecupper(\theta_S) (r \ell\herm + {(r \ell\herm)}\trans) \right)}, \\
            dJ &= - \re{\strictvec(a b\herm - {(a b\herm)}\trans)}, \\
            dN &= - \re{\strictvec({(r \ell\herm)}\trans - r \ell\herm)}, \\
            dG &= - \re{\matvec(a \ell\herm + {(r b\herm)}\trans)}, \text{ and} \\
            dP &= - \re{\matvec(a \ell\herm - {(r b\herm)}\trans)}.
        \end{aligned}
    \end{equation}
\end{theorem}

\begin{proof}
    We commence by showing the statement for the $dE$ component of $\nabla_\theta \sigma_j(\tfunc(s) - \tfunc_\theta(s))$.
    To begin, we give an important argument which analogously be applied to the gradient components of $J$ and $R$.
    Let $\varepsilon > 0$ be given and consider the parameter update $\theta_E + \varepsilon e_i$ at the $i$-th relevant component of $\theta$.
    By abuse of notation, we additionally use this notation to mean $\theta + \varepsilon e_i$ by padding $e_i$ with zeros such that the affected parameter component remains the same.
    Next, we apply the updated parameter to the \ac{PH} system $\Sigma_\msc{ph}(\theta + \varepsilon e_i)$, whereafter we compute the updated $E$ matrix as
    \begin{equation}\label{eq:sobmor-e-matrix-update}
        \begin{aligned}
            E(\theta + \varepsilon e_i) &= {\vecupper(\theta_E + \varepsilon e_i)}\trans \vecupper(\theta_E + \varepsilon e_i) \\
             &= {\vecupper(\theta_E)}\trans \vecupper(\theta_E) + \varepsilon \left( {\vecupper(e_i)}\trans \vecupper(\theta_E) + {\vecupper(\theta_E)}\trans \vecupper(e_i) \right) + \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i) \\
             &= E(\theta) + \varepsilon \left( {\vecupper(e_i)}\trans \vecupper(\theta_E) + {\vecupper(\theta_E)}\trans \vecupper(e_i) \right) + \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i).
        \end{aligned}
    \end{equation}
    We now want to take the first derivative of~\eqref{eq:sobmor-e-matrix-update} with respect to $\varepsilon$.
    For this, we define the intermediary matrix $\Delta_E \coloneqq {\vecupper(e_i)}\trans \vecupper(\theta_E) + {\vecupper(\theta_E)}\trans \vecupper(e_i)$ and compute the transfer function
    \begin{equation}\label{eq:sobmor-e-variation}
        \begin{aligned}
            \tfunc_{\theta + \varepsilon e_i}(s) &= {(G + P)}\trans Q {\left( s \left( E + \varepsilon \Delta_E + \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i) \right) - (J - R) Q \right)}\inv (G - P) + (S - N) \\
             &= {(G + P)}\trans Q \underbrace{{\left( F + s \varepsilon \Delta_E + s \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i) \right)}\inv}_{(*)} (G - P) + (S - N).
        \end{aligned}
    \end{equation}
    To compute $(*)$ in Equation~\eqref{eq:sobmor-e-variation}, we use the fact that if a matrix $M \in \bb{R}^{n \times n}$ satisfies $\norm{M}{} < 1$, then $(\id + M)$ has the inverse ${(\id + M)}\inv = \sum\limits_{m = 0}^\infty {(-M)}^m$.
    Now, if $\varepsilon > 0$ is small enough, the necessary condition $\norm{\id + s \varepsilon F\inv \Delta_E + s \varepsilon^2 F\inv {\vecupper(e_i)}\trans \vecupper(e_i)}{} < 1$ is satisfied, and after applying the regularity of the matrix stencil $F$ we calculate
    \begin{equation}\label{eq:sobmor-e-stencil-inversion}
        \begin{alignedat}{3}
            &(*) &= &{\left( F \left( \id + s \varepsilon F\inv \Delta_E + s \varepsilon^2 {\vecupper(e_i)}\trans \vecupper(e_i) \right) \right)}\inv \\
            & &= &{\left( \id + s \varepsilon F\inv \Delta_E + s \varepsilon^2 F\inv {\vecupper(e_i)}\trans \vecupper(e_i) \right)}\inv F\inv \\
            & &= &\sum\limits_{m = 0}^\infty {\left( - s \varepsilon F\inv \Delta_E - s \varepsilon^2 F\inv {\vecupper(e_i)}\trans \vecupper(e_i) \right)}^m F\inv.
        \end{alignedat}
    \end{equation}
    Moreover, we can express the Taylor series expansion of the parametrized transfer function $\tfunc_\theta$ in terms of~\eqref{eq:sobmor-e-stencil-inversion}.
    This results in the following expression of the updated transfer function
    \begin{equation}\label{eq:sobmor-e-taylor}
        \tfunc_{\theta + \varepsilon e_i}(s) = \tfunc_\theta(s) - {(G + P)}\trans Q \sum\limits_{m = 1}^\infty {\left( s \varepsilon F\inv \Delta_E + s \varepsilon^2 F\inv {\vecupper(e_i)}\trans \vecupper(e_i) \right)}^m F\inv (G - P).
    \end{equation}
    As argued in~\cite[Theorem~1]{Schwerdtner2023}, the map $\varepsilon \mapsto \sigma_i\left( \tfunc(s) - \tfunc_{\theta + \varepsilon e_i}(s) \right)$ is differentiable for $\varepsilon > 0$ small enough as a consequence of~\cite{Lancaster1964}, thereby admitting the following expression of the $dE$ gradient component
    \begin{equation}\label{eq:sobmor-e-trace-form}
        \begin{aligned}
            \res{\frac{\dif}{\dif \varepsilon} \sigma_i(\tfunc(s) - \tfunc_{\theta + \varepsilon e_i}(s))}{\varepsilon = 0} &= \re{s \ell\herm {(G + P)}\trans Q F\inv \Delta_E F\inv (G - P) r} \\
             \overset{(\msc{a})}&{=} \re{s \trace{F\inv (G - P) r \ell\herm {(G + P)}\trans Q F\inv \Delta_E}} \\
             &= \re{s \trace{a b\herm \left( {\vecupper(e_i)}\trans \vecupper(\theta_E) + {\vecupper(\theta_E)}\trans \vecupper(e_i) \right)}} \\
             \overset{(\msc{b})}&{=} \re{s \trace{\left( {(a b\herm)}\trans {\vecupper(\theta_E)}\trans + a b\herm {\vecupper(\theta_E)}\trans \right) \vecupper(e_i)}} \\
             \overset{(\msc{c})}&{=} \re{s \uppervec\left( \vecupper(\theta_E) \bigl( a b\herm + {(a b\herm)}\trans \bigr) \right)}.
        \end{aligned}
    \end{equation}
    In this transformation, we have applied the following arguments:
    \begin{enumerate}[label= (\scshape{\alph*}):]
        \item For a vector product $u\herm v$ we can equivalently compute the trace $\trace{v u\herm}$.
        \item For the first summand of $\Delta_E$ we exploit that the trace operator is cyclic and invariant under transpositions. Thus we calculate $\trace{A B C\trans D} = \trace{D\trans C B\trans A\trans} = \trace{{(A B)}\trans D\trans C}$.
        \item Lemma~\ref{lem:v2m-function-trace}.
    \end{enumerate}
    We can compute the component updates for the matrices $J$ and $R$ in a similar way, requiring only slight changes in the variational terms $dJ$ and $dR$ corresponding to $\Delta_E$.
    As an example, we calculate the final steps of the equivalent of~\eqref{eq:sobmor-e-trace-form} for $J$ with $\Delta_J = {\vecstrict(e_i)}\trans - \vecstrict(e_i)$:
    \begin{equation*}
        \begin{aligned}
            \res{\frac{\dif}{\dif \varepsilon} \sigma_i(\tfunc(s) - \tfunc_{\theta + \varepsilon e_i}(s))}{\varepsilon = 0} &= - \re{\trace{a b\herm \Delta_J}} = - \re{\trace{a b\herm \left({\vecstrict(e_i)}\trans - \vecstrict(e_i)\right)}} \\
             \overset{(\msc{b})}&{=} - \re{\trace{\left( {(a b\herm)}\trans - a b\herm \right) \vecstrict(e_i)}} \overset{(\msc{c})}{=} - \re{\strictvec\left( a b\herm - {(a b\herm)}\trans \right)}.
        \end{aligned}
    \end{equation*}

    For the two matrices $G$ and $P$ we can follow an easier procedure.
    As before, we compute the matrix $G$ for the updated parameter $\theta_G + \varepsilon e_i$ and define the intermediary $\Delta_G \coloneqq \vecmat(e_i)$.
    Similarly to~\eqref{eq:sobmor-e-variation}, we thereafter calculate the updated transfer function
    \begin{equation*}
        \begin{aligned}
            \tfunc_{\theta + \varepsilon e_i}(s) &= {(G + \varepsilon \Delta_G + P)}\trans Q F\inv (G + \varepsilon \Delta_G - P) + (S - N) \\
             &= \tfunc_\theta(s) + \varepsilon {\Delta_G}\trans Q F\inv (G - P) + \varepsilon {(G + P)}\trans Q F\inv \Delta_G + \varepsilon^2 {\Delta_G}\trans Q F\inv \Delta_G.
        \end{aligned}
    \end{equation*}
    By considering the Taylor expansion of $\tfunc_{\theta + \varepsilon e_i}$ as in~\eqref{eq:sobmor-e-taylor}, we can observe that the derivative of the singular value $\sigma_i(\tfunc(s) - \tfunc_{\theta + \varepsilon e_i}(s))$ equals
    \begin{equation*}
        \begin{aligned}
            \res{\frac{\dif}{\dif \varepsilon} \sigma_i(\tfunc(s) - \tfunc_{\theta + \varepsilon e_i}(s))}{\varepsilon = 0} &= - \re{\ell\herm \left( {\Delta_G}\trans Q F\inv (G - P) + {(G + P)}\trans Q F\inv \Delta_G \right) r} \\
            \overset{(\msc{a})}&{=} - \re{\trace{Q F\inv (G - P) r \ell\herm {\Delta_G}\trans + r \ell\herm {(G + P)}\trans Q F\inv \Delta_G}} \\
            &= - \re{\trace{a \ell\herm {\Delta_G}\trans + r b\herm \Delta_G}} \overset{(\msc{b})}{=} - \re{\trace{\left( {\left( a \ell\herm \right)}\trans + r b\herm \right) \Delta_G}} \\
            &= -\re{\trace{\left( {\left( a \ell\herm \right)}\trans + r b\herm \right) \vecmat(e_i)}}\overset{(\msc{c})}{=} -\re{\matvec\left( a \ell\herm + {\left( r b\herm \right)}\trans \right)}.
        \end{aligned}
    \end{equation*}
    The result follows analogously for $P$ with a change of signs.

    Finally, the computation for the matrices $S$ and $N$ is a combination of the two previously described procedures.
    With $\Delta_S \coloneqq {\vecupper(e_i)}\trans \vecupper(\theta_S) + {\vecupper(\Delta_S)}\trans \vecupper(e_i)$ we can repeat~\eqref{eq:sobmor-e-trace-form} for $S$ and calculate
    \begin{equation*}
        \begin{aligned}
            \res{\frac{\dif}{\dif \varepsilon} \sigma_i(\tfunc(s) - \tfunc_{\theta + \varepsilon e_i}(s))}{\varepsilon = 0} &= - \re{\trace{r l\herm \Delta_S}} \\
            &= - \re{\trace{\left( {\left( r l\herm \right)}\trans + r l\herm \right) {\vecupper(\theta_S)}\trans \vecupper(e_i)}} \\
            &= - \re{\uppervec\left( \vecupper(\theta_S) \left( r l\herm + {\left( r l\herm \right)}\trans \right) \right)}.
        \end{aligned}
    \end{equation*}
    These equalities can be applied to $N$ in an analogous manner to prove the theorem.
\end{proof}

For the computation of the objective functional's gradient, we now follow the procedure in~\cite{SV2023} and use an inner \ac{BFGS} iteration for each value of $\gamma$.
The authors terminate the outer iteration as soon as the objective $\mcl{L}$ returns a value greater than zero because at that point the $\mcl{H}_\infty$ norm has to be larger than or equal to $\gamma$, see~\cite[Section~3.2.2]{SV2023}.

The selection processes of both the sample points $\mcl{S}$ and the thresholds $\gamma$ can be varied.
Among the options for the sample points are $\iu \omega \in \iu \bb{R}$, where $\omega$ are logarithmically spaced points on the real axis, and an adaptive sampling method adjusted for the value of the objective function between two sample points as proposed in~\cite{SV2021}.
While the former is much simpler in concept, it suffers from the fact that the sample set cannot adapt to the problem-specific sampling ranges.
In turn, the weighting in the outer gradient~\eqref{eq:outer-sobmor-objective-gradient} cannot adequately adjust to the underlying problem.
Adaptive sampling like it was described in~\cite{SV2021} is a proposed solution to this drawback, increasing the number of samples in areas of the sampling range where large errors are introduced.
In this case, the outer gradient~\eqref{eq:outer-sobmor-objective-gradient} includes more error terms the more samples are added in the badly approximated ranges, resulting in an additional weighting of the gradient at these sample points.
