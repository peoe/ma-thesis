\section{Abstract Limitations of Linear MOR}\label{sec:limitations-linear-mor}

This section is not directly linked to the systemtheoretic scope of this thesis, however it demonstrates an important motivation for the next chapter.
We thus think it reasonable to make a small detour and talk about the direct limitations of linear MOR for \acp{PDE}.
In effect, we want to transpose this motivation onto the problem inherent in this manuscript, however no such rigorous results as we present in this section are available as of the time of writing.

To make clear what we want to demonstrate here, we commence Subsection~\ref{subsec:greedy-algorithms} with the description of the greedy algorithms, a technique for reducing parametric \acp{PDE} without inputs or outputs.
We then give the context for a few model problems which serve as a good intuition on how the model we consider for reduction can impact the effectiveness of the reduction procedure in Subsection~\ref{subsec:kolmogorov-n-width}.

\subsection{Greedy Approximation Algorithms}\label{subsec:greedy-algorithms}

Unlike the truncation-based or interpolation-based methods presented in Section~\ref{sec:system-mor}, greedy algorithms rely on a solution snapshot based linear subspace approximation; cf.~\cite{Grepl2005, Rozza2008, Buffa2012}.
In this setting, solution snapshots are defined as the solutions $u(\mu_i)$ to a \ac{PDE} of interest for differing parameters $\mu_i$.
Using a more heuristic approach we are going to choose a momentarily optimal basis element to improve the approximation in each iteration instead of computing a singular value decomposition over the set of all gathered snapshots like in Proper Orthogonal Decomposition; cf.~\cite{Pinkus1985, Pinnau2008}.

The idea is as follows: given a sample set of parameters $\mcl{S}_r$ and an associated reduced linear subspace $V_r$ spanned by the solution snapshots $u(\mu_i), i = 1, \dots, r$, we want to choose the next sample parameter $\mu_{r + 1}$ such that the snapshot $u_r(\mu_{r + 1})$ induces the largest error over all possible sampling parameters with repect to the full order solution $u(\mu_{r + 1})$.
Choosing this parameter amounts to adding the basis element to our reduced subspace $V_{r + 1} = \lspn{V_r, u(\mu_{r + 1})}$ which created the largest error in the approximation with the previous basis, hence maximizing the reduction in error over all sampling parameters every step.
A more rigorous statement of the greedy algorithm structure is given in Algorithm~\ref{alg:greedy}.

Though this greedy approach must not result in an optimal subspace, the approximations have been shown to be of good quality.
Besides the approximative properties however, the particular convergence rates of greedy algorithms heavily depend on the underlying full order model.

\begin{algorithm}\label{alg:greedy}
    \caption{Greedy Algorithm; cf.~\cite[Algorithm~1]{Buffa2012}}
    \KwData{Parameter domain $\mcl{P}$, full order solution map $u \colon \mcl{P} \to \mcl{X}$}
    $\mu_1 \coloneqq \argmax\limits_{\mu \in \mcl{P}} \norm{u(\mu)}{\mcl{X}}, V_1 \coloneqq \lspn{u(\mu_1)} \subseteq \mcl{X}$\;
    Compute the reduced solution map $u_1 \colon \mcl{P} \to V_1$\;
    $r \coloneqq 1$\;
    \While{$\argmax\limits_{\mu \in \mcl{P}} \norm{u(\mu) - u_r(\mu)}{\mcl{X}} \geq \tol$}{
        $\mu_{r + 1} \coloneqq \argmax\limits_{\mu \in \mcl{P}} \norm{u(\mu) - u_r(\mu)}{\mcl{X}}, V_{r + 1} \coloneqq \lspn{V_r, u(\mu_{r + 1})} \subseteq \mcl{X}$\;
        Compute the reduced solution map $u_{r + 1} \colon \mcl{P} \to V_{r + 1}$\;
        $r \coloneqq r + 1$\;
    }
\end{algorithm}

\subsection[The Kolmogorov N-Width]{The Kolmogorov {$N$}-Width}\label{subsec:kolmogorov-n-width}

With the greedy basis $u_1 = u(\mu_1), \dots, u_r = u(\mu_r)$ computed from Algorithm~\ref{alg:greedy} we form a matrix $U$ spanning the reduced space $V_r$ with its columns
\begin{equation*}
    U = \left( u_1, \dots, u_r \right) \in \bb{R}^{n \times r}.
\end{equation*}
Using this matrix to project the full order space $\bb{R}^n$ onto the linear subspace $\bb{R}^r$ we can transform an exemplary full order discretized \ac{PDE} such as $A u = L$ with $A \in \bb{R}^{n \times n}, L \in \bb{R}^n$ to a reduced model by projecting the operators
\begin{equation*}
    \tilde{A} = U\trans A U,\quad \tilde{L} = U\trans L.
\end{equation*}

In practice, the matrix $U$ only serves as a projection onto a subspace with a much smaller dimension $d \ll n$ meaning that the resulting reduced solution $u_r$ will always introduce some error $\norm{u - u_r}{}$ with respect to the full order solution $u$ in some appropriate norm.
To bound the error of the reduced solutions we have to consider so-called N-widths; cf.~\cite{Pinkus1985}.
For linear approximations of problems defined on a parameter space $\mcl{P}$ we can provide an upper bound of the approximation quality of any reduced space $V_N$ with respect to the full order space $V$ by considering the \emph{Kolmogorov N-width} of a solution manifold $\mcl{M} = \iset{u(\mu)}{\mu \in \mcl{P}}$
\begin{equation}
    \kolm{\mcl{M}} \coloneqq \inf\limits_{\substack{V_N \subseteq V,\\\dim{(V_N)} = N}} \sup\limits_{v \in \mcl{M}} \inf\limits_{u_r \in V_N} \norm{v - u_r}{}.
\end{equation}
For greedy algorithms, a fast decay of the N-width is observed; cf.~\cite{Binev2011, DeVore2013}.
It can be shown that if the solutoins $u(\mu)$ depend analytically on $\mu$, then there exist constants $a, c > 0$ such that the Kolmogorov N-width can be bounded by
\begin{equation*}
    \kolm{\mcl{M}} \leq c \exp{(-N^\alpha)}.
\end{equation*}

However, not all problems exhibit this analytical solution manifold behaviour.
When considering advection dominated probles such as~\cite[Section~5.1]{Ohlberger2016} the upper bound on the Kolmogorov N-width instead turns into a lower bound limiting the decay
\begin{equation*}
    \kolm{\mcl{M}} \geq \frac{1}{2} N^{- \frac{1}{2}}.
\end{equation*}
Similarly, the hypberloic wave equation only decays with a lower bound of
\begin{equation*}
    \kolm{\mcl{M}} \geq \frac{1}{4} N^{- \frac{1}{2}}.
\end{equation*}
This slow decay becomes problematic when constructiong reduced models because one would require much larger reduced orders to make the error w.\ r.\ t.\ the full order model small enough.
The drawback of these increasing bases is the simultaneous increase in computational cost reducing the (online) speedup gained from reducing the model.
The consequence of these findings is immediate: To preserve the speedup obtained from order reduction we need to incorporate nonlinear parts into the reduced models.
We highlight some of these nonlinear infering approaches in Chapter~\ref{chap:inferring-models}.
